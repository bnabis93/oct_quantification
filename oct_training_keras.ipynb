{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/bono/.pyenv/versions/3.5.5/envs/gpuTest/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/bono/.pyenv/versions/3.5.5/envs/gpuTest/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/bono/.pyenv/versions/3.5.5/envs/gpuTest/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/bono/.pyenv/versions/3.5.5/envs/gpuTest/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:522: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/bono/.pyenv/versions/3.5.5/envs/gpuTest/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/bono/.pyenv/versions/3.5.5/envs/gpuTest/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/job:localhost/replica:0/task:0/device:GPU:0']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import configparser\n",
    "import shutil\n",
    "\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras import backend as K\n",
    "from keras.utils.vis_utils import plot_model as plotn\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "from keras import models\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, './lib_keras/')\n",
    "from help_functions import *\n",
    "\n",
    "from lib_keras.model_lib import *\n",
    "#function to obtain data for training/testing (validation)\n",
    "from extract_patches import get_data_training\n",
    "\n",
    "print(K.tensorflow_backend._get_available_gpus())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "temp = 11579\n",
    "print(temp % 1156)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## custom U-net Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " path data : ./data/hdf5_data/inha_oct_5classes/\n",
      " save_folder : result\n",
      " name_experiment : 09_no_contrast_64_Attn_notNaive_hybridTest\n",
      " num_epoches : 50\n",
      " batch_size : 8\n",
      "\n",
      "OrderedDict([((0, 0, 0), 0), ((255, 0, 0), 1), ((0, 255, 0), 2), ((0, 0, 255), 3), ((255, 255, 0), 4)])\n",
      "5\n",
      "./result/09_no_contrast_64_Attn_notNaive_hybridTest/\n"
     ]
    }
   ],
   "source": [
    "config = configparser.RawConfigParser()\n",
    "config.read('configuration.txt')\n",
    "#patch to the datasets\n",
    "path_data = config.get('data paths', 'path_local')\n",
    "#Experiment name\n",
    "save_folder = config.get('experiment name','result_save_path')\n",
    "name_experiment = config.get('experiment name', 'name')\n",
    "\n",
    "#training settings\n",
    "num_epochs = int(config.get('training settings', 'num_epochs'))\n",
    "batch_size = int(config.get('training settings', 'batch_size'))\n",
    "\n",
    "print(' path data : {}\\n save_folder : {}\\n name_experiment : {}\\n num_epoches : {}\\n batch_size : {}\\n'.format(path_data, save_folder,\\\n",
    "                                                                                                               name_experiment,\\\n",
    "                                                                                                               num_epochs,batch_size))\n",
    "'''\n",
    "5 class\n",
    "class01 (255,0,0), Red , RNFL\n",
    "class02 (0,255,0), Green, other layers\n",
    "class03 (0,0,255), Blue, RPE\n",
    "class04 (255,255,0), Yellow, LC\n",
    "\n",
    "'''\n",
    "mapping = OrderedDict()\n",
    "mapping[(0,0,0)] = 0\n",
    "mapping[(255,0,0)] = 1\n",
    "mapping[(0,255,0)] = 2\n",
    "mapping[(0,0,255)] = 3\n",
    "mapping[(255,255,0)] = 4\n",
    "\n",
    "\n",
    "print(mapping)\n",
    "print(len(mapping))\n",
    "print('./'+save_folder+'/'+name_experiment+'/')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isdir('./'+save_folder+'/'+name_experiment+'/') == False:\n",
    "    os.mkdir('./'+save_folder+'/'+name_experiment+'/')\n",
    "else:\n",
    "    print('already exist the folder in this path : {}'.format('./'+save_folder+'/'+name_experiment+'/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./result/09_no_contrast_64_Attn_notNaive_hybridTest/configuration.txt'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_data + config.get('data paths', 'train_groundTruth')\n",
    "\n",
    "# copy configuration\n",
    "\n",
    "shutil.copyfile('./configuration.txt', './'+save_folder+'/'+name_experiment+'/configuration.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract patch for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " train path : ./data/hdf5_data/inha_oct_5classes/inha_oct_5classes_train.hdf5\n",
      " train label path : ./data/hdf5_data/inha_oct_5classes/inha_oct_5classes_groundTruth_train.hdf5\n",
      " patch height : 64 patch width : 64\n",
      " num subimgs : 200000 \t inside FOV : False \t save path : result/09_no_contrast_64_Attn_notNaive_hybridTest\n"
     ]
    }
   ],
   "source": [
    "print(' train path : {}\\n train label path : {}\\n patch height : {} patch width : {}\\n\\\n",
    " num subimgs : {} \\t inside FOV : {} \\t save path : {}'.format(path_data + config.get('data paths', 'train_imgs_original'),\\\n",
    "                                                       path_data + config.get('data paths', 'train_groundTruth'),\\\n",
    "                                                       int(config.get('data attributes', 'patch_height')),\\\n",
    "                                                       int(config.get('data attributes', 'patch_width')),\\\n",
    "                                                       int(config.get('training settings', 'num_subimgs')),\\\n",
    "                                                       config.getboolean('training settings', 'inside_FOV'),\\\n",
    "                                                       save_folder+'/'+name_experiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already exist the folder in this path : result/09_no_contrast_64_Attn_notNaive_hybridTest\n",
      "number of subimages :  200000\n",
      "[DEBUG] shape of train_imgs_original :  (40, 3, 500, 760)\n",
      "[DEBUG] shape of train_imgs_label :  (40, 3, 500, 760)\n",
      "[group images func] prev data shape  : (40, 3, 500, 760)\n",
      "[group images func] after data shape :  (40, 500, 760, 3)\n",
      "[group images func] first total image :  (500, 3800, 3)\n",
      "[group images func] final total image :  (4500, 3800, 3)\n",
      "data shape :  (4500, 3800, 3)\n",
      "<PIL.Image.Image image mode=RGB size=3800x4500 at 0x7F77D0C9E320>\n",
      "file name :  ./result/09_no_contrast_64_Attn_notNaive_hybridTest/imgs_train\n",
      "[group images func] prev data shape  : (40, 3, 500, 760)\n",
      "[group images func] after data shape :  (40, 500, 760, 3)\n",
      "[group images func] first total image :  (500, 3800, 3)\n",
      "[group images func] final total image :  (4500, 3800, 3)\n",
      "data shape :  (4500, 3800, 3)\n",
      "<PIL.Image.Image image mode=RGB size=3800x4500 at 0x7F77D0C9E320>\n",
      "file name :  ./result/09_no_contrast_64_Attn_notNaive_hybridTest/imgs_labels\n",
      "[DEBUG] normalize shape :  (40, 1, 500, 760)\n",
      "[DEBUG] i normalize shape :  (1, 500, 760)\n",
      "[group images func] prev data shape  : (40, 1, 500, 760)\n",
      "[group images func] after data shape :  (40, 500, 760, 1)\n",
      "[group images func] first total image :  (500, 3800, 1)\n",
      "[group images func] final total image :  (4500, 3800, 1)\n",
      "data shape :  (4500, 3800, 1)\n",
      "<PIL.Image.Image image mode=L size=3800x4500 at 0x7F77D0C9E3C8>\n",
      "file name :  ./result/09_no_contrast_64_Attn_notNaive_hybridTest/preprocessed\n",
      "\n",
      "\n",
      "[get_data_training] preprocessed image shape :  (40, 1, 500, 760)\n",
      "\n",
      "[get_data_training] preprocessed mask shape :  (40, 3, 500, 760)\n",
      "mask maximum val :  255.0\n",
      "[get_data_training] preprocessed2 image shape :  (40, 1, 500, 760)\n",
      "\n",
      "\n",
      "[padding] pad h size : 12\t pad w size : 8\n",
      "\n",
      "\n",
      "[padding] imgs shape : (40, 500, 760, 1)\t labels shape : (40, 500, 760, 3)\n",
      "\n",
      "\n",
      "[padding] pad imgs shape : (40, 512, 768, 1)\t pad labels shape : (40, 512, 768, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "extract patches:   0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[final pad] imgs shape : (40, 512, 768, 1)\t labels shape : (40, 512, 768, 3)\n",
      "\n",
      "\n",
      "[final transpose] imgs shape : (40, 1, 512, 768)\t labels shape : (40, 3, 512, 768)\n",
      "\n",
      "\n",
      "[get_data_training] train images/masks shape : (40, 1, 512, 768)\n",
      "[get_data_training] train images range (min-max) [0.0 , 1.0] \n",
      "[get_data_training] train masks are within 0-1\n",
      "\n",
      "\n",
      "\n",
      "[extract random] num of class :  5\n",
      "[extract random] full image shape : (40, 1, 512, 768)\n",
      "[extract random] full masks shape : (40, 3, 512, 768)\n",
      "[extract random] patches shape : (200000, 1, 64, 64)\n",
      "[extract random] patches masks shape : (200000, 5, 64, 64)\n",
      "[extract random] patches per full image : 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "extract patches: 100%|██████████| 40/40 [07:46<00:00, 11.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[After patch] mask shape :  (200000, 5, 64, 64)\n",
      "[Augmentation function] patches shape :  (200000, 1, 64, 64)\n",
      "[Augmentation function] augmentation patches shape :  (200000, 64, 64, 1)\n",
      "[Augmentation function] augmentation patches masks shape :  (200000, 64, 64, 5)\n",
      "\n",
      "\n",
      "[get_data_training] train PATCHES images/masks shape : (200000, 1, 64, 64)\n",
      "[get_data_training] train PATCHES images range (min-max): 0.0 - 1.0\n",
      "[get_data_training] patches_imgs_train : (200000, 1, 64, 64)\n",
      "[group images func] prev data shape  : (50, 1, 64, 64)\n",
      "[group images func] after data shape :  (50, 64, 64, 1)\n",
      "[group images func] first total image :  (64, 320, 1)\n",
      "[group images func] final total image :  (704, 320, 1)\n",
      "data shape :  (704, 320, 1)\n",
      "<PIL.Image.Image image mode=L size=320x704 at 0x7F77D01EED68>\n",
      "file name :  ./result/09_no_contrast_64_Attn_notNaive_hybridTest/train_patch_img\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "patches_imgs_train, patches_masks_train, class_freq_tabel = get_data_training(\n",
    "    train_imgs_original = path_data + config.get('data paths', 'train_imgs_original'),\n",
    "    train_groudTruth = path_data + config.get('data paths', 'train_groundTruth'),  #masks\n",
    "    patch_height = int(config.get('data attributes', 'patch_height')),\n",
    "    patch_width = int(config.get('data attributes', 'patch_width')),\n",
    "    num_subimgs = int(config.get('training settings', 'num_subimgs')),\n",
    "    label_mapping = mapping,\n",
    "    inside_FOV = config.getboolean('training settings', 'inside_FOV'), #select the patches only inside the FOV  (default == True)\n",
    "    save_path = save_folder+'/'+name_experiment\n",
    ")\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class_freq_tabel \n",
    "\n",
    "class_freq_tabel['frequency_0'] = class_freq_tabel['class_0'] / (class_freq_tabel['frequency_0'] * (64*64))\n",
    "class_freq_tabel['frequency_1'] = class_freq_tabel['class_1'] / (class_freq_tabel['frequency_1'] * (64*64))\n",
    "class_freq_tabel['frequency_2'] = class_freq_tabel['class_2'] / (class_freq_tabel['frequency_2'] * (64*64))\n",
    "class_freq_tabel['frequency_3'] = class_freq_tabel['class_3'] / (class_freq_tabel['frequency_3'] * (64*64))\n",
    "class_freq_tabel['frequency_4'] = class_freq_tabel['class_4'] / (class_freq_tabel['frequency_4'] * (64*64))\n",
    "\n",
    "\n",
    "#df = pd.DataFrame()\n",
    "#df['']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_0</th>\n",
       "      <th>class_1</th>\n",
       "      <th>class_2</th>\n",
       "      <th>class_3</th>\n",
       "      <th>class_4</th>\n",
       "      <th>frequency_0</th>\n",
       "      <th>frequency_1</th>\n",
       "      <th>frequency_2</th>\n",
       "      <th>frequency_3</th>\n",
       "      <th>frequency_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17448586</td>\n",
       "      <td>833468</td>\n",
       "      <td>1236180</td>\n",
       "      <td>361563</td>\n",
       "      <td>600203</td>\n",
       "      <td>0.851982</td>\n",
       "      <td>0.0406967</td>\n",
       "      <td>0.0603604</td>\n",
       "      <td>0.0176544</td>\n",
       "      <td>0.0293068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16344932</td>\n",
       "      <td>1465679</td>\n",
       "      <td>1611246</td>\n",
       "      <td>354105</td>\n",
       "      <td>704038</td>\n",
       "      <td>0.798092</td>\n",
       "      <td>0.0715664</td>\n",
       "      <td>0.0786741</td>\n",
       "      <td>0.0172903</td>\n",
       "      <td>0.0343769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16308644</td>\n",
       "      <td>1950908</td>\n",
       "      <td>1073159</td>\n",
       "      <td>309579</td>\n",
       "      <td>837710</td>\n",
       "      <td>0.796321</td>\n",
       "      <td>0.0952592</td>\n",
       "      <td>0.0524003</td>\n",
       "      <td>0.0151162</td>\n",
       "      <td>0.0409038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16513670</td>\n",
       "      <td>1580175</td>\n",
       "      <td>1375593</td>\n",
       "      <td>363730</td>\n",
       "      <td>646832</td>\n",
       "      <td>0.806332</td>\n",
       "      <td>0.077157</td>\n",
       "      <td>0.0671676</td>\n",
       "      <td>0.0177603</td>\n",
       "      <td>0.0315836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16180743</td>\n",
       "      <td>1368451</td>\n",
       "      <td>1936633</td>\n",
       "      <td>369123</td>\n",
       "      <td>625050</td>\n",
       "      <td>0.790075</td>\n",
       "      <td>0.0668189</td>\n",
       "      <td>0.0945622</td>\n",
       "      <td>0.0180236</td>\n",
       "      <td>0.03052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>16405506</td>\n",
       "      <td>1912908</td>\n",
       "      <td>1411984</td>\n",
       "      <td>359261</td>\n",
       "      <td>390341</td>\n",
       "      <td>0.80105</td>\n",
       "      <td>0.0934037</td>\n",
       "      <td>0.0689445</td>\n",
       "      <td>0.017542</td>\n",
       "      <td>0.0190596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>17382997</td>\n",
       "      <td>1537100</td>\n",
       "      <td>944199</td>\n",
       "      <td>319895</td>\n",
       "      <td>295809</td>\n",
       "      <td>0.848779</td>\n",
       "      <td>0.0750537</td>\n",
       "      <td>0.0461035</td>\n",
       "      <td>0.0156199</td>\n",
       "      <td>0.0144438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16848405</td>\n",
       "      <td>1048581</td>\n",
       "      <td>1843015</td>\n",
       "      <td>387345</td>\n",
       "      <td>352654</td>\n",
       "      <td>0.822676</td>\n",
       "      <td>0.0512002</td>\n",
       "      <td>0.089991</td>\n",
       "      <td>0.0189133</td>\n",
       "      <td>0.0172194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>17113817</td>\n",
       "      <td>1231636</td>\n",
       "      <td>1441430</td>\n",
       "      <td>428188</td>\n",
       "      <td>264929</td>\n",
       "      <td>0.835636</td>\n",
       "      <td>0.0601385</td>\n",
       "      <td>0.0703823</td>\n",
       "      <td>0.0209076</td>\n",
       "      <td>0.012936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>17189934</td>\n",
       "      <td>1386271</td>\n",
       "      <td>1286013</td>\n",
       "      <td>392786</td>\n",
       "      <td>224996</td>\n",
       "      <td>0.839352</td>\n",
       "      <td>0.067689</td>\n",
       "      <td>0.0627936</td>\n",
       "      <td>0.019179</td>\n",
       "      <td>0.0109861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>16074802</td>\n",
       "      <td>2089333</td>\n",
       "      <td>1554752</td>\n",
       "      <td>414542</td>\n",
       "      <td>346571</td>\n",
       "      <td>0.784902</td>\n",
       "      <td>0.102018</td>\n",
       "      <td>0.0759156</td>\n",
       "      <td>0.0202413</td>\n",
       "      <td>0.0169224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>16501561</td>\n",
       "      <td>1939513</td>\n",
       "      <td>1210968</td>\n",
       "      <td>400496</td>\n",
       "      <td>427462</td>\n",
       "      <td>0.80574</td>\n",
       "      <td>0.0947028</td>\n",
       "      <td>0.0591293</td>\n",
       "      <td>0.0195555</td>\n",
       "      <td>0.0208722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>16242746</td>\n",
       "      <td>2233176</td>\n",
       "      <td>1296211</td>\n",
       "      <td>364152</td>\n",
       "      <td>343715</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>0.109042</td>\n",
       "      <td>0.0632916</td>\n",
       "      <td>0.0177809</td>\n",
       "      <td>0.016783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>16519590</td>\n",
       "      <td>1123811</td>\n",
       "      <td>2212660</td>\n",
       "      <td>344031</td>\n",
       "      <td>279908</td>\n",
       "      <td>0.806621</td>\n",
       "      <td>0.0548736</td>\n",
       "      <td>0.10804</td>\n",
       "      <td>0.0167984</td>\n",
       "      <td>0.0136674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>16878127</td>\n",
       "      <td>1638430</td>\n",
       "      <td>1301786</td>\n",
       "      <td>383190</td>\n",
       "      <td>278467</td>\n",
       "      <td>0.824127</td>\n",
       "      <td>0.0800015</td>\n",
       "      <td>0.0635638</td>\n",
       "      <td>0.0187104</td>\n",
       "      <td>0.013597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15823881</td>\n",
       "      <td>2605440</td>\n",
       "      <td>1275246</td>\n",
       "      <td>390979</td>\n",
       "      <td>384454</td>\n",
       "      <td>0.77265</td>\n",
       "      <td>0.127219</td>\n",
       "      <td>0.0622679</td>\n",
       "      <td>0.0190908</td>\n",
       "      <td>0.0187722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16505586</td>\n",
       "      <td>1914696</td>\n",
       "      <td>1366954</td>\n",
       "      <td>404458</td>\n",
       "      <td>288306</td>\n",
       "      <td>0.805937</td>\n",
       "      <td>0.093491</td>\n",
       "      <td>0.0667458</td>\n",
       "      <td>0.0197489</td>\n",
       "      <td>0.0140774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>16868154</td>\n",
       "      <td>1481393</td>\n",
       "      <td>1511297</td>\n",
       "      <td>414188</td>\n",
       "      <td>204968</td>\n",
       "      <td>0.82364</td>\n",
       "      <td>0.0723336</td>\n",
       "      <td>0.0737938</td>\n",
       "      <td>0.020224</td>\n",
       "      <td>0.0100082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>16034188</td>\n",
       "      <td>2277061</td>\n",
       "      <td>1271939</td>\n",
       "      <td>468242</td>\n",
       "      <td>428570</td>\n",
       "      <td>0.782919</td>\n",
       "      <td>0.111185</td>\n",
       "      <td>0.0621064</td>\n",
       "      <td>0.0228634</td>\n",
       "      <td>0.0209263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>16723626</td>\n",
       "      <td>1587402</td>\n",
       "      <td>1369173</td>\n",
       "      <td>422579</td>\n",
       "      <td>377220</td>\n",
       "      <td>0.816583</td>\n",
       "      <td>0.0775099</td>\n",
       "      <td>0.0668542</td>\n",
       "      <td>0.0206337</td>\n",
       "      <td>0.0184189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>17532286</td>\n",
       "      <td>955251</td>\n",
       "      <td>1138720</td>\n",
       "      <td>368479</td>\n",
       "      <td>485264</td>\n",
       "      <td>0.856069</td>\n",
       "      <td>0.0466431</td>\n",
       "      <td>0.0556016</td>\n",
       "      <td>0.0179921</td>\n",
       "      <td>0.0236945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>16224556</td>\n",
       "      <td>2083956</td>\n",
       "      <td>1391793</td>\n",
       "      <td>353922</td>\n",
       "      <td>425773</td>\n",
       "      <td>0.792215</td>\n",
       "      <td>0.101756</td>\n",
       "      <td>0.0679586</td>\n",
       "      <td>0.0172813</td>\n",
       "      <td>0.0207897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>16624088</td>\n",
       "      <td>1651434</td>\n",
       "      <td>1300088</td>\n",
       "      <td>415473</td>\n",
       "      <td>488917</td>\n",
       "      <td>0.811723</td>\n",
       "      <td>0.0806364</td>\n",
       "      <td>0.0634809</td>\n",
       "      <td>0.0202868</td>\n",
       "      <td>0.0238729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>16365121</td>\n",
       "      <td>2094543</td>\n",
       "      <td>1130740</td>\n",
       "      <td>421424</td>\n",
       "      <td>468172</td>\n",
       "      <td>0.799078</td>\n",
       "      <td>0.102273</td>\n",
       "      <td>0.0552119</td>\n",
       "      <td>0.0205773</td>\n",
       "      <td>0.02286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>16135511</td>\n",
       "      <td>2071921</td>\n",
       "      <td>1155848</td>\n",
       "      <td>348445</td>\n",
       "      <td>768275</td>\n",
       "      <td>0.787867</td>\n",
       "      <td>0.101168</td>\n",
       "      <td>0.0564379</td>\n",
       "      <td>0.0170139</td>\n",
       "      <td>0.0375134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>17162318</td>\n",
       "      <td>1463132</td>\n",
       "      <td>1092161</td>\n",
       "      <td>385047</td>\n",
       "      <td>377342</td>\n",
       "      <td>0.838004</td>\n",
       "      <td>0.071442</td>\n",
       "      <td>0.0533282</td>\n",
       "      <td>0.0188011</td>\n",
       "      <td>0.0184249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>16787652</td>\n",
       "      <td>905934</td>\n",
       "      <td>1818987</td>\n",
       "      <td>460641</td>\n",
       "      <td>506786</td>\n",
       "      <td>0.81971</td>\n",
       "      <td>0.0442351</td>\n",
       "      <td>0.0888177</td>\n",
       "      <td>0.0224922</td>\n",
       "      <td>0.0247454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>16173484</td>\n",
       "      <td>1995145</td>\n",
       "      <td>1410118</td>\n",
       "      <td>362516</td>\n",
       "      <td>538737</td>\n",
       "      <td>0.789721</td>\n",
       "      <td>0.0974192</td>\n",
       "      <td>0.0688534</td>\n",
       "      <td>0.017701</td>\n",
       "      <td>0.0263055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>16589281</td>\n",
       "      <td>1802035</td>\n",
       "      <td>1213492</td>\n",
       "      <td>464497</td>\n",
       "      <td>410695</td>\n",
       "      <td>0.810023</td>\n",
       "      <td>0.08799</td>\n",
       "      <td>0.0592525</td>\n",
       "      <td>0.0226805</td>\n",
       "      <td>0.0200535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>16597112</td>\n",
       "      <td>1850925</td>\n",
       "      <td>1206747</td>\n",
       "      <td>418257</td>\n",
       "      <td>406959</td>\n",
       "      <td>0.810406</td>\n",
       "      <td>0.0903772</td>\n",
       "      <td>0.0589232</td>\n",
       "      <td>0.0204227</td>\n",
       "      <td>0.019871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>16176344</td>\n",
       "      <td>2011807</td>\n",
       "      <td>1444277</td>\n",
       "      <td>461820</td>\n",
       "      <td>385752</td>\n",
       "      <td>0.789861</td>\n",
       "      <td>0.0982328</td>\n",
       "      <td>0.0705213</td>\n",
       "      <td>0.0225498</td>\n",
       "      <td>0.0188355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>16485451</td>\n",
       "      <td>2155853</td>\n",
       "      <td>1279653</td>\n",
       "      <td>319015</td>\n",
       "      <td>240028</td>\n",
       "      <td>0.804954</td>\n",
       "      <td>0.105266</td>\n",
       "      <td>0.0624831</td>\n",
       "      <td>0.0155769</td>\n",
       "      <td>0.0117201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>16769625</td>\n",
       "      <td>1628407</td>\n",
       "      <td>1320110</td>\n",
       "      <td>458841</td>\n",
       "      <td>303017</td>\n",
       "      <td>0.818829</td>\n",
       "      <td>0.0795121</td>\n",
       "      <td>0.0644585</td>\n",
       "      <td>0.0224043</td>\n",
       "      <td>0.0147958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>16333385</td>\n",
       "      <td>2040199</td>\n",
       "      <td>1037823</td>\n",
       "      <td>357587</td>\n",
       "      <td>711006</td>\n",
       "      <td>0.797529</td>\n",
       "      <td>0.0996191</td>\n",
       "      <td>0.050675</td>\n",
       "      <td>0.0174603</td>\n",
       "      <td>0.0347171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>16008266</td>\n",
       "      <td>1947471</td>\n",
       "      <td>1264841</td>\n",
       "      <td>377747</td>\n",
       "      <td>881675</td>\n",
       "      <td>0.781654</td>\n",
       "      <td>0.0950914</td>\n",
       "      <td>0.0617598</td>\n",
       "      <td>0.0184447</td>\n",
       "      <td>0.0430505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>16445629</td>\n",
       "      <td>1957002</td>\n",
       "      <td>1158499</td>\n",
       "      <td>353415</td>\n",
       "      <td>565455</td>\n",
       "      <td>0.803009</td>\n",
       "      <td>0.0955567</td>\n",
       "      <td>0.0565673</td>\n",
       "      <td>0.0172566</td>\n",
       "      <td>0.0276101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>16806936</td>\n",
       "      <td>1028575</td>\n",
       "      <td>1246101</td>\n",
       "      <td>316415</td>\n",
       "      <td>1081973</td>\n",
       "      <td>0.820651</td>\n",
       "      <td>0.0502234</td>\n",
       "      <td>0.0608448</td>\n",
       "      <td>0.01545</td>\n",
       "      <td>0.0528307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>15903330</td>\n",
       "      <td>2064389</td>\n",
       "      <td>1435415</td>\n",
       "      <td>495782</td>\n",
       "      <td>581084</td>\n",
       "      <td>0.77653</td>\n",
       "      <td>0.1008</td>\n",
       "      <td>0.0700886</td>\n",
       "      <td>0.0242081</td>\n",
       "      <td>0.0283732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>17341684</td>\n",
       "      <td>860964</td>\n",
       "      <td>1177164</td>\n",
       "      <td>307215</td>\n",
       "      <td>792973</td>\n",
       "      <td>0.846762</td>\n",
       "      <td>0.0420393</td>\n",
       "      <td>0.0574787</td>\n",
       "      <td>0.0150007</td>\n",
       "      <td>0.0387194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>15978663</td>\n",
       "      <td>2250641</td>\n",
       "      <td>1457350</td>\n",
       "      <td>396123</td>\n",
       "      <td>397223</td>\n",
       "      <td>0.780208</td>\n",
       "      <td>0.109895</td>\n",
       "      <td>0.0711597</td>\n",
       "      <td>0.0193419</td>\n",
       "      <td>0.0193957</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     class_0  class_1  class_2 class_3  class_4 frequency_0 frequency_1  \\\n",
       "0   17448586   833468  1236180  361563   600203    0.851982   0.0406967   \n",
       "1   16344932  1465679  1611246  354105   704038    0.798092   0.0715664   \n",
       "2   16308644  1950908  1073159  309579   837710    0.796321   0.0952592   \n",
       "3   16513670  1580175  1375593  363730   646832    0.806332    0.077157   \n",
       "4   16180743  1368451  1936633  369123   625050    0.790075   0.0668189   \n",
       "5   16405506  1912908  1411984  359261   390341     0.80105   0.0934037   \n",
       "6   17382997  1537100   944199  319895   295809    0.848779   0.0750537   \n",
       "7   16848405  1048581  1843015  387345   352654    0.822676   0.0512002   \n",
       "8   17113817  1231636  1441430  428188   264929    0.835636   0.0601385   \n",
       "9   17189934  1386271  1286013  392786   224996    0.839352    0.067689   \n",
       "10  16074802  2089333  1554752  414542   346571    0.784902    0.102018   \n",
       "11  16501561  1939513  1210968  400496   427462     0.80574   0.0947028   \n",
       "12  16242746  2233176  1296211  364152   343715    0.793103    0.109042   \n",
       "13  16519590  1123811  2212660  344031   279908    0.806621   0.0548736   \n",
       "14  16878127  1638430  1301786  383190   278467    0.824127   0.0800015   \n",
       "15  15823881  2605440  1275246  390979   384454     0.77265    0.127219   \n",
       "16  16505586  1914696  1366954  404458   288306    0.805937    0.093491   \n",
       "17  16868154  1481393  1511297  414188   204968     0.82364   0.0723336   \n",
       "18  16034188  2277061  1271939  468242   428570    0.782919    0.111185   \n",
       "19  16723626  1587402  1369173  422579   377220    0.816583   0.0775099   \n",
       "20  17532286   955251  1138720  368479   485264    0.856069   0.0466431   \n",
       "21  16224556  2083956  1391793  353922   425773    0.792215    0.101756   \n",
       "22  16624088  1651434  1300088  415473   488917    0.811723   0.0806364   \n",
       "23  16365121  2094543  1130740  421424   468172    0.799078    0.102273   \n",
       "24  16135511  2071921  1155848  348445   768275    0.787867    0.101168   \n",
       "25  17162318  1463132  1092161  385047   377342    0.838004    0.071442   \n",
       "26  16787652   905934  1818987  460641   506786     0.81971   0.0442351   \n",
       "27  16173484  1995145  1410118  362516   538737    0.789721   0.0974192   \n",
       "28  16589281  1802035  1213492  464497   410695    0.810023     0.08799   \n",
       "29  16597112  1850925  1206747  418257   406959    0.810406   0.0903772   \n",
       "30  16176344  2011807  1444277  461820   385752    0.789861   0.0982328   \n",
       "31  16485451  2155853  1279653  319015   240028    0.804954    0.105266   \n",
       "32  16769625  1628407  1320110  458841   303017    0.818829   0.0795121   \n",
       "33  16333385  2040199  1037823  357587   711006    0.797529   0.0996191   \n",
       "34  16008266  1947471  1264841  377747   881675    0.781654   0.0950914   \n",
       "35  16445629  1957002  1158499  353415   565455    0.803009   0.0955567   \n",
       "36  16806936  1028575  1246101  316415  1081973    0.820651   0.0502234   \n",
       "37  15903330  2064389  1435415  495782   581084     0.77653      0.1008   \n",
       "38  17341684   860964  1177164  307215   792973    0.846762   0.0420393   \n",
       "39  15978663  2250641  1457350  396123   397223    0.780208    0.109895   \n",
       "\n",
       "   frequency_2 frequency_3 frequency_4  \n",
       "0    0.0603604   0.0176544   0.0293068  \n",
       "1    0.0786741   0.0172903   0.0343769  \n",
       "2    0.0524003   0.0151162   0.0409038  \n",
       "3    0.0671676   0.0177603   0.0315836  \n",
       "4    0.0945622   0.0180236     0.03052  \n",
       "5    0.0689445    0.017542   0.0190596  \n",
       "6    0.0461035   0.0156199   0.0144438  \n",
       "7     0.089991   0.0189133   0.0172194  \n",
       "8    0.0703823   0.0209076    0.012936  \n",
       "9    0.0627936    0.019179   0.0109861  \n",
       "10   0.0759156   0.0202413   0.0169224  \n",
       "11   0.0591293   0.0195555   0.0208722  \n",
       "12   0.0632916   0.0177809    0.016783  \n",
       "13     0.10804   0.0167984   0.0136674  \n",
       "14   0.0635638   0.0187104    0.013597  \n",
       "15   0.0622679   0.0190908   0.0187722  \n",
       "16   0.0667458   0.0197489   0.0140774  \n",
       "17   0.0737938    0.020224   0.0100082  \n",
       "18   0.0621064   0.0228634   0.0209263  \n",
       "19   0.0668542   0.0206337   0.0184189  \n",
       "20   0.0556016   0.0179921   0.0236945  \n",
       "21   0.0679586   0.0172813   0.0207897  \n",
       "22   0.0634809   0.0202868   0.0238729  \n",
       "23   0.0552119   0.0205773     0.02286  \n",
       "24   0.0564379   0.0170139   0.0375134  \n",
       "25   0.0533282   0.0188011   0.0184249  \n",
       "26   0.0888177   0.0224922   0.0247454  \n",
       "27   0.0688534    0.017701   0.0263055  \n",
       "28   0.0592525   0.0226805   0.0200535  \n",
       "29   0.0589232   0.0204227    0.019871  \n",
       "30   0.0705213   0.0225498   0.0188355  \n",
       "31   0.0624831   0.0155769   0.0117201  \n",
       "32   0.0644585   0.0224043   0.0147958  \n",
       "33    0.050675   0.0174603   0.0347171  \n",
       "34   0.0617598   0.0184447   0.0430505  \n",
       "35   0.0565673   0.0172566   0.0276101  \n",
       "36   0.0608448     0.01545   0.0528307  \n",
       "37   0.0700886   0.0242081   0.0283732  \n",
       "38   0.0574787   0.0150007   0.0387194  \n",
       "39   0.0711597   0.0193419   0.0193957  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_freq_tabel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_class_00</th>\n",
       "      <th>avg_class_01</th>\n",
       "      <th>avg_class_02</th>\n",
       "      <th>avg_class_03</th>\n",
       "      <th>avg_class_04</th>\n",
       "      <th>avg_freq_00</th>\n",
       "      <th>avg_freq_01</th>\n",
       "      <th>avg_freq_02</th>\n",
       "      <th>avg_freq_03</th>\n",
       "      <th>avg_freq_04</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.65588e+07</td>\n",
       "      <td>1.70063e+06</td>\n",
       "      <td>1.35526e+06</td>\n",
       "      <td>387377</td>\n",
       "      <td>477983</td>\n",
       "      <td>0.808533</td>\n",
       "      <td>0.0830383</td>\n",
       "      <td>0.0661748</td>\n",
       "      <td>0.0189149</td>\n",
       "      <td>0.023339</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  avg_class_00 avg_class_01 avg_class_02 avg_class_03 avg_class_04  \\\n",
       "0  1.65588e+07  1.70063e+06  1.35526e+06       387377       477983   \n",
       "\n",
       "  avg_freq_00 avg_freq_01 avg_freq_02 avg_freq_03 avg_freq_04  \n",
       "0    0.808533   0.0830383   0.0661748   0.0189149    0.023339  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "average_class_freq = pd.DataFrame(columns=['avg_class_00', 'avg_class_01', 'avg_class_02','avg_class_03','avg_class_04', 'avg_freq_00','avg_freq_01','avg_freq_02','avg_freq_03','avg_freq_04'])\n",
    "\n",
    "average_class_freq.loc[0,'avg_class_00'] = class_freq_tabel['class_0'].mean()\n",
    "average_class_freq.loc[0,'avg_class_01'] = class_freq_tabel['class_1'].mean()\n",
    "average_class_freq.loc[0,'avg_class_02'] = class_freq_tabel['class_2'].mean()\n",
    "average_class_freq.loc[0,'avg_class_03'] = class_freq_tabel['class_3'].mean()\n",
    "average_class_freq.loc[0,'avg_class_04'] = class_freq_tabel['class_4'].mean()\n",
    "\n",
    "average_class_freq.loc[0,'avg_freq_00'] = class_freq_tabel['frequency_0'].mean()\n",
    "average_class_freq.loc[0,'avg_freq_01'] = class_freq_tabel['frequency_1'].mean()\n",
    "average_class_freq.loc[0,'avg_freq_02'] = class_freq_tabel['frequency_2'].mean()\n",
    "average_class_freq.loc[0,'avg_freq_03'] = class_freq_tabel['frequency_3'].mean()\n",
    "average_class_freq.loc[0,'avg_freq_04'] = class_freq_tabel['frequency_4'].mean()\n",
    "\n",
    "average_class_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_freq_tabel.to_csv('./'+save_folder+'/'+name_experiment+'/'+'all_class_imbalance.csv', encoding='utf-8')\n",
    "average_class_freq.to_csv('./'+save_folder+'/'+name_experiment+'/'+'avg_class_imbalance.csv', encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 1, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "patches_masks_train_class00 = patches_masks_train[:,0,:,:] \n",
    "patches_masks_train_class00 = np.expand_dims(patches_masks_train_class00,1)\n",
    "patches_masks_train_class01 = patches_masks_train[:,1,:,:] \n",
    "patches_masks_train_class01 = np.expand_dims(patches_masks_train_class01,1)\n",
    "patches_masks_train_class02 = patches_masks_train[:,2,:,:] \n",
    "patches_masks_train_class02 = np.expand_dims(patches_masks_train_class02,1)\n",
    "patches_masks_train_class03 = patches_masks_train[:,3,:,:] \n",
    "patches_masks_train_class03 = np.expand_dims(patches_masks_train_class03,1)\n",
    "patches_masks_train_class04 = patches_masks_train[:,4,:,:] \n",
    "patches_masks_train_class04 = np.expand_dims(patches_masks_train_class04,1)\n",
    "\n",
    "\n",
    "print(np.shape(patches_masks_train_class00))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[group images func] prev data shape  : (40, 1, 64, 64)\n",
      "[group images func] after data shape :  (40, 64, 64, 1)\n",
      "[group images func] first total image :  (64, 320, 1)\n",
      "[group images func] final total image :  (576, 320, 1)\n",
      "data shape :  (576, 320, 1)\n",
      "<PIL.Image.Image image mode=L size=320x576 at 0x7F77D019C5C0>\n",
      "file name :  ./result/09_no_contrast_64_Attn_notNaive_hybridTest/sample_input_imgs\n",
      "[group images func] prev data shape  : (40, 1, 64, 64)\n",
      "[group images func] after data shape :  (40, 64, 64, 1)\n",
      "[group images func] first total image :  (64, 320, 1)\n",
      "[group images func] final total image :  (576, 320, 1)\n",
      "data shape :  (576, 320, 1)\n",
      "<PIL.Image.Image image mode=L size=320x576 at 0x7F77D019C550>\n",
      "file name :  ./result/09_no_contrast_64_Attn_notNaive_hybridTest/sample_input_masks_claass0\n",
      "[group images func] prev data shape  : (40, 1, 64, 64)\n",
      "[group images func] after data shape :  (40, 64, 64, 1)\n",
      "[group images func] first total image :  (64, 320, 1)\n",
      "[group images func] final total image :  (576, 320, 1)\n",
      "data shape :  (576, 320, 1)\n",
      "<PIL.Image.Image image mode=L size=320x576 at 0x7F77D019C5C0>\n",
      "file name :  ./result/09_no_contrast_64_Attn_notNaive_hybridTest/sample_input_masks_claass1\n",
      "[group images func] prev data shape  : (40, 1, 64, 64)\n",
      "[group images func] after data shape :  (40, 64, 64, 1)\n",
      "[group images func] first total image :  (64, 320, 1)\n",
      "[group images func] final total image :  (576, 320, 1)\n",
      "data shape :  (576, 320, 1)\n",
      "<PIL.Image.Image image mode=L size=320x576 at 0x7F77D019C588>\n",
      "file name :  ./result/09_no_contrast_64_Attn_notNaive_hybridTest/sample_input_masks_claass2\n",
      "[group images func] prev data shape  : (40, 1, 64, 64)\n",
      "[group images func] after data shape :  (40, 64, 64, 1)\n",
      "[group images func] first total image :  (64, 320, 1)\n",
      "[group images func] final total image :  (576, 320, 1)\n",
      "data shape :  (576, 320, 1)\n",
      "<PIL.Image.Image image mode=L size=320x576 at 0x7F77D019C5C0>\n",
      "file name :  ./result/09_no_contrast_64_Attn_notNaive_hybridTest/sample_input_masks_claass3\n",
      "[group images func] prev data shape  : (40, 1, 64, 64)\n",
      "[group images func] after data shape :  (40, 64, 64, 1)\n",
      "[group images func] first total image :  (64, 320, 1)\n",
      "[group images func] final total image :  (576, 320, 1)\n",
      "data shape :  (576, 320, 1)\n",
      "<PIL.Image.Image image mode=L size=320x576 at 0x7F77D019C630>\n",
      "file name :  ./result/09_no_contrast_64_Attn_notNaive_hybridTest/sample_input_masks_claass4\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUAAAAJACAAAAADZE+DrAAABy0lEQVR4nO3aQQqDMBRAwej975wuanuAPCHYzoDgRgmPD4bgGAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA8PuO3Qt4qnm1O3cv5KHmdQlYCRgJGAkYCRgJGAkYzCFgJmAkYDEFzARMpoDRFDAScM33HFXASMBIwEjANfNzI2Ak4Bpf4bsIGAkYCRgJGAkYCRgJuOizERQwEnDVNYICLnsXFHCdP1TvIGBwDAEzAYtDwEzASMBIwEjASMBIwEjASMBIwEjASMBIwEjASMBIwEjASMBIwEjASMBIwEjASMBIQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAGC7Izw76wt+wbl7AU8nYCRgJGAkYCRgJGAkYCRgJGAkYCRgJGAkYCRgJGAkYCRgVAL++2n+GMMEZgJGAkYCRgJGAkYCRimgjaAJzASMBIwEjASMBIwEjASMBIwEjASMBIxaQKcJJrAScLO5ewG7mcBIQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP7EC5snCtx0kirhAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=320x576 at 0x7F77D019C630>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_sample = min(patches_imgs_train.shape[0],40)\n",
    "visualize(group_images(patches_imgs_train[0:N_sample,:,:,:],5),'./'+save_folder+'/'+name_experiment+'/'+\"sample_input_imgs\")#.show()\n",
    "visualize(group_images(patches_masks_train_class00[0:N_sample,:,:,:],5),'./'+save_folder+'/'+name_experiment+'/'+\"sample_input_masks_claass0\")#.show()\n",
    "visualize(group_images(patches_masks_train_class01[0:N_sample,:,:,:],5),'./'+save_folder+'/'+name_experiment+'/'+\"sample_input_masks_claass1\")#.show()\n",
    "visualize(group_images(patches_masks_train_class02[0:N_sample,:,:,:],5),'./'+save_folder+'/'+name_experiment+'/'+\"sample_input_masks_claass2\")#.show()\n",
    "visualize(group_images(patches_masks_train_class03[0:N_sample,:,:,:],5),'./'+save_folder+'/'+name_experiment+'/'+\"sample_input_masks_claass3\")#.show()\n",
    "visualize(group_images(patches_masks_train_class04[0:N_sample,:,:,:],5),'./'+save_folder+'/'+name_experiment+'/'+\"sample_input_masks_claass4\")#.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape :  (200000, 1, 64, 64)\n",
      "n_ch : 1 patch_h : 64 patch_w : 64\n",
      "input shape :  (None, 1, 64, 64)\n",
      "\n",
      "gating shape : (None, 128, 4, 4), conv4 shape : (None, 64, 8, 8)\n",
      "shape x,g  (None, 64, 8, 8) (None, 128, 4, 4)\n",
      "inter shape :   128\n",
      "stride x : 1 stride y : 1\n",
      "theta_x shape :  (None, 128, 4, 4)\n",
      "upsample_g shape :  (None, 128, 4, 4)\n",
      "\n",
      "attn1 shape : (None, 64, 8, 8) center shape : (None, 128, 4, 4) \n",
      "\n",
      "attn1 shape : (None, 64, 8, 8) up1 shape : (None, 128, 8, 8)\n",
      "shape x,g  (None, 64, 16, 16) (None, 128, 8, 8)\n",
      "inter shape :   64\n",
      "stride x : 1 stride y : 1\n",
      "theta_x shape :  (None, 64, 8, 8)\n",
      "upsample_g shape :  (None, 64, 8, 8)\n",
      "shape x,g  (None, 32, 32, 32) (None, 128, 16, 16)\n",
      "inter shape :   64\n",
      "stride x : 1 stride y : 1\n",
      "theta_x shape :  (None, 64, 16, 16)\n",
      "upsample_g shape :  (None, 64, 16, 16)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1, 64, 64)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 64, 64)   320         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 64, 64)   256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 64, 64)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 64, 64)   9248        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 64, 64)   256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 64, 64)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 32)   0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 32)   9248        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 32)   128         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 32)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 32)   9248        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 32)   128         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 32)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 32, 16, 16)   0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 64, 16, 16)   18496       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 64, 16, 16)   64          conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 64, 16, 16)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 64, 16, 16)   36928       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 64, 16, 16)   64          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 64, 16, 16)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 64, 8, 8)     0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 64, 8, 8)     36928       max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 64, 8, 8)     32          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 64, 8, 8)     0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 64, 8, 8)     36928       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 64, 8, 8)     32          conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 64, 8, 8)     0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 64, 4, 4)     0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 128, 4, 4)    73856       max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 128, 4, 4)    16          conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 128, 4, 4)    0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 128, 4, 4)    147584      activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 128, 4, 4)    16          conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 128, 4, 4)    0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "gating01_conv (Conv2D)          (None, 128, 4, 4)    16512       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "gating01_bn (BatchNormalization (None, 128, 4, 4)    16          gating01_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "gating01_act (Activation)       (None, 128, 4, 4)    0           gating01_bn[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 128, 4, 4)    16512       gating01_act[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "g_upattn01 (Conv2DTranspose)    (None, 128, 4, 4)    147584      conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "xlattn01 (Conv2D)               (None, 128, 4, 4)    32896       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 128, 4, 4)    0           g_upattn01[0][0]                 \n",
      "                                                                 xlattn01[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 128, 4, 4)    0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "psiattn01 (Conv2D)              (None, 1, 4, 4)      129         activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 1, 4, 4)      0           psiattn01[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 1, 8, 8)      0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "psi_upattn01 (Lambda)           (None, 64, 8, 8)     0           up_sampling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "q_attnattn01 (Multiply)         (None, 64, 8, 8)     0           psi_upattn01[0][0]               \n",
      "                                                                 activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "q_attn_convattn01 (Conv2D)      (None, 64, 8, 8)     4160        q_attnattn01[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 64, 8, 8)     73792       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "q_attn_bnattn01 (BatchNormaliza (None, 64, 8, 8)     32          q_attn_convattn01[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 128, 8, 8)    0           conv2d_transpose_1[0][0]         \n",
      "                                                                 q_attn_bnattn01[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "gating02_conv (Conv2D)          (None, 128, 8, 8)    16512       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "gating02_bn (BatchNormalization (None, 128, 8, 8)    32          gating02_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "gating02_act (Activation)       (None, 128, 8, 8)    0           gating02_bn[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 64, 8, 8)     8256        gating02_act[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "g_upattn02 (Conv2DTranspose)    (None, 64, 8, 8)     36928       conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "xlattn02 (Conv2D)               (None, 64, 8, 8)     16448       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 64, 8, 8)     0           g_upattn02[0][0]                 \n",
      "                                                                 xlattn02[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 64, 8, 8)     0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "psiattn02 (Conv2D)              (None, 1, 8, 8)      65          activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 1, 8, 8)      0           psiattn02[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 1, 16, 16)    0           activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "psi_upattn02 (Lambda)           (None, 64, 16, 16)   0           up_sampling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "q_attnattn02 (Multiply)         (None, 64, 16, 16)   0           psi_upattn02[0][0]               \n",
      "                                                                 activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "q_attn_convattn02 (Conv2D)      (None, 64, 16, 16)   4160        q_attnattn02[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 64, 16, 16)   73792       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "q_attn_bnattn02 (BatchNormaliza (None, 64, 16, 16)   64          q_attn_convattn02[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 128, 16, 16)  0           conv2d_transpose_2[0][0]         \n",
      "                                                                 q_attn_bnattn02[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "gating03_conv (Conv2D)          (None, 128, 16, 16)  16512       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "gating03_bn (BatchNormalization (None, 128, 16, 16)  64          gating03_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "gating03_act (Activation)       (None, 128, 16, 16)  0           gating03_bn[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 64, 16, 16)   8256        gating03_act[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "g_upattn03 (Conv2DTranspose)    (None, 64, 16, 16)   36928       conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "xlattn03 (Conv2D)               (None, 64, 16, 16)   8256        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 64, 16, 16)   0           g_upattn03[0][0]                 \n",
      "                                                                 xlattn03[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 64, 16, 16)   0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "psiattn03 (Conv2D)              (None, 1, 16, 16)    65          activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 1, 16, 16)    0           psiattn03[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)  (None, 1, 32, 32)    0           activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "psi_upattn03 (Lambda)           (None, 32, 32, 32)   0           up_sampling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "q_attnattn03 (Multiply)         (None, 32, 32, 32)   0           psi_upattn03[0][0]               \n",
      "                                                                 activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "q_attn_convattn03 (Conv2D)      (None, 32, 32, 32)   1056        q_attnattn03[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 32, 32, 32)   36896       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "q_attn_bnattn03 (BatchNormaliza (None, 32, 32, 32)   128         q_attn_convattn03[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 64, 32, 32)   0           conv2d_transpose_3[0][0]         \n",
      "                                                                 q_attn_bnattn03[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTrans (None, 32, 64, 64)   18464       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 64, 64, 64)   0           conv2d_transpose_4[0][0]         \n",
      "                                                                 activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 5, 64, 64)    325         concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 5, 4096)      0           conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "permute_1 (Permute)             (None, 4096, 5)      0           reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 4096, 5)      0           permute_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 954,616\n",
      "Trainable params: 953,952\n",
      "Non-trainable params: 664\n",
      "__________________________________________________________________________________________________\n",
      "Check: final output of the network:\n",
      "(None, 4096, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "41736"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_ch = patches_imgs_train.shape[1]\n",
    "patch_height = patches_imgs_train.shape[2]\n",
    "patch_width = patches_imgs_train.shape[3]\n",
    "print('shape : ',patches_imgs_train.shape)\n",
    "print('n_ch : {} patch_h : {} patch_w : {}'.format(n_ch, patch_height, patch_width))\n",
    "#model = naive_attn_unet(n_ch, patch_height, patch_width)  #the U-net model\n",
    "#model = unet_norm(n_ch, patch_height, patch_width,len(mapping))\n",
    "#model = naive_attn_unet(n_ch, patch_height, patch_width,len(mapping))\n",
    "\n",
    "#model = bigger_unet_norm(n_ch, patch_height, patch_width,len(mapping))\n",
    "#model = bigger_naive_attn_unet(n_ch, patch_height, patch_width,len(mapping))\n",
    "model = attn_unet(n_ch, patch_height, patch_width,len(mapping))\n",
    "#model = attn_reg_test(n_ch, patch_height, patch_width,len(mapping))\n",
    "\n",
    "print (\"Check: final output of the network:\")\n",
    "print (model.output_shape)\n",
    "\n",
    "#plot(model, to_file= './'+save_folder+'/'+name_experiment+'/' +name_experiment+ '_model.png')   #check how the model looks like\n",
    "#plot(model, to_file= name_experiment+'/'+name_experiment + '_model.png')   #check how the model looks like\n",
    "\n",
    "json_string = model.to_json()\n",
    "open('./'+save_folder+'/'+name_experiment+'/' +name_experiment+'_architecture.json', 'w').write(json_string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[training session] mask unet func patch mask shape :q  (200000, 5, 64, 64)\n",
      "[training session] After mask unet func patch mask shape :  (200000, 4096, 5)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Keras provides a set of functions called callbacks: \n",
    "you can think of callbacks as events that will be triggered at certain training states. \n",
    "The callback we need for checkpointing is the ModelCheckpoint \n",
    "which provides all the features we need according to the checkpointing strategy we adopted in our example\n",
    "'''\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='./'+save_folder+'/'+name_experiment+'/best_weights.h5', verbose=1, monitor='val_loss', mode='auto', save_best_only=True) #save at each epoch if the validation decreased\n",
    "\n",
    "print('[training session] mask unet func patch mask shape :q ',patches_masks_train.shape)\n",
    "patches_masks_train = np.reshape(patches_masks_train, (patches_masks_train.shape[0], len(mapping), patch_height* patch_width))\n",
    "patches_masks_train = np.transpose(patches_masks_train,(0,2,1))\n",
    "#patches_masks_train = masks_Unet(patches_masks_train)  #reduce memory consumption\n",
    "print('[training session] After mask unet func patch mask shape : ',patches_masks_train.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 160000 samples, validate on 40000 samples\n",
      "Epoch 1/50\n",
      "160000/160000 [==============================] - 1850s 12ms/step - loss: 38.1978 - acc: 0.8585 - val_loss: 30.3643 - val_acc: 0.8899\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 30.36429, saving model to ./result/09_no_contrast_64_Attn_notNaive_hybridTest/best_weights.h5\n",
      "Epoch 2/50\n",
      "160000/160000 [==============================] - 1711s 11ms/step - loss: 20.9866 - acc: 0.9188 - val_loss: 19.9273 - val_acc: 0.9256\n",
      "\n",
      "Epoch 00002: val_loss improved from 30.36429 to 19.92726, saving model to ./result/09_no_contrast_64_Attn_notNaive_hybridTest/best_weights.h5\n",
      "Epoch 3/50\n",
      "160000/160000 [==============================] - 1818s 11ms/step - loss: 15.2500 - acc: 0.9386 - val_loss: 16.0494 - val_acc: 0.9363\n",
      "\n",
      "Epoch 00003: val_loss improved from 19.92726 to 16.04942, saving model to ./result/09_no_contrast_64_Attn_notNaive_hybridTest/best_weights.h5\n",
      "Epoch 4/50\n",
      "160000/160000 [==============================] - 1724s 11ms/step - loss: 13.1729 - acc: 0.9440 - val_loss: 14.8028 - val_acc: 0.9376\n",
      "\n",
      "Epoch 00004: val_loss improved from 16.04942 to 14.80275, saving model to ./result/09_no_contrast_64_Attn_notNaive_hybridTest/best_weights.h5\n",
      "Epoch 5/50\n",
      "160000/160000 [==============================] - 1916s 12ms/step - loss: 12.0364 - acc: 0.9467 - val_loss: 15.1812 - val_acc: 0.9385\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 14.80275\n",
      "Epoch 6/50\n",
      "160000/160000 [==============================] - 1766s 11ms/step - loss: 11.2188 - acc: 0.9486 - val_loss: 13.8065 - val_acc: 0.9410\n",
      "\n",
      "Epoch 00006: val_loss improved from 14.80275 to 13.80647, saving model to ./result/09_no_contrast_64_Attn_notNaive_hybridTest/best_weights.h5\n",
      "Epoch 7/50\n",
      "160000/160000 [==============================] - 1905s 12ms/step - loss: 10.6379 - acc: 0.9498 - val_loss: 14.7506 - val_acc: 0.9402\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 13.80647\n",
      "Epoch 8/50\n",
      "160000/160000 [==============================] - 1747s 11ms/step - loss: 10.1376 - acc: 0.9510 - val_loss: 12.8906 - val_acc: 0.9427\n",
      "\n",
      "Epoch 00008: val_loss improved from 13.80647 to 12.89057, saving model to ./result/09_no_contrast_64_Attn_notNaive_hybridTest/best_weights.h5\n",
      "Epoch 9/50\n",
      "160000/160000 [==============================] - 1930s 12ms/step - loss: 9.7542 - acc: 0.9519 - val_loss: 13.4999 - val_acc: 0.9426\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 12.89057\n",
      "Epoch 10/50\n",
      "160000/160000 [==============================] - 1730s 11ms/step - loss: 9.4444 - acc: 0.9527 - val_loss: 13.0217 - val_acc: 0.9421\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 12.89057\n",
      "Epoch 11/50\n",
      "160000/160000 [==============================] - 1921s 12ms/step - loss: 9.1785 - acc: 0.9534 - val_loss: 12.0046 - val_acc: 0.9435\n",
      "\n",
      "Epoch 00011: val_loss improved from 12.89057 to 12.00463, saving model to ./result/09_no_contrast_64_Attn_notNaive_hybridTest/best_weights.h5\n",
      "Epoch 12/50\n",
      "160000/160000 [==============================] - 1726s 11ms/step - loss: 8.9367 - acc: 0.9539 - val_loss: 12.3254 - val_acc: 0.9445\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 12.00463\n",
      "Epoch 13/50\n",
      "160000/160000 [==============================] - 1886s 12ms/step - loss: 8.7454 - acc: 0.9545 - val_loss: 11.6072 - val_acc: 0.9445\n",
      "\n",
      "Epoch 00013: val_loss improved from 12.00463 to 11.60716, saving model to ./result/09_no_contrast_64_Attn_notNaive_hybridTest/best_weights.h5\n",
      "Epoch 14/50\n",
      "160000/160000 [==============================] - 1726s 11ms/step - loss: 8.5780 - acc: 0.9550 - val_loss: 11.6080 - val_acc: 0.9442\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 11.60716\n",
      "Epoch 15/50\n",
      "160000/160000 [==============================] - 1914s 12ms/step - loss: 8.4250 - acc: 0.9554 - val_loss: 11.9911 - val_acc: 0.9441\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 11.60716\n",
      "Epoch 16/50\n",
      "160000/160000 [==============================] - 1738s 11ms/step - loss: 8.2771 - acc: 0.9557 - val_loss: 11.4322 - val_acc: 0.9448\n",
      "\n",
      "Epoch 00016: val_loss improved from 11.60716 to 11.43224, saving model to ./result/09_no_contrast_64_Attn_notNaive_hybridTest/best_weights.h5\n",
      "Epoch 17/50\n",
      "160000/160000 [==============================] - 1883s 12ms/step - loss: 8.1604 - acc: 0.9562 - val_loss: 11.7253 - val_acc: 0.9452\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 11.43224\n",
      "Epoch 18/50\n",
      "160000/160000 [==============================] - 1740s 11ms/step - loss: 8.0266 - acc: 0.9565 - val_loss: 11.7292 - val_acc: 0.9458\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 11.43224\n",
      "Epoch 19/50\n",
      "160000/160000 [==============================] - 1887s 12ms/step - loss: 7.9273 - acc: 0.9568 - val_loss: 12.5714 - val_acc: 0.9448\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 11.43224\n",
      "Epoch 20/50\n",
      "160000/160000 [==============================] - 1758s 11ms/step - loss: 7.8298 - acc: 0.9571 - val_loss: 12.4730 - val_acc: 0.9442\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 11.43224\n",
      "Epoch 21/50\n",
      "160000/160000 [==============================] - 1895s 12ms/step - loss: 7.7236 - acc: 0.9575 - val_loss: 11.7080 - val_acc: 0.9428\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 11.43224\n",
      "Epoch 22/50\n",
      "160000/160000 [==============================] - 1726s 11ms/step - loss: 7.6416 - acc: 0.9577 - val_loss: 11.2762 - val_acc: 0.9465\n",
      "\n",
      "Epoch 00022: val_loss improved from 11.43224 to 11.27620, saving model to ./result/09_no_contrast_64_Attn_notNaive_hybridTest/best_weights.h5\n",
      "Epoch 23/50\n",
      "160000/160000 [==============================] - 1891s 12ms/step - loss: 7.5637 - acc: 0.9580 - val_loss: 11.6083 - val_acc: 0.9463\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 11.27620\n",
      "Epoch 24/50\n",
      "160000/160000 [==============================] - 1732s 11ms/step - loss: 7.4831 - acc: 0.9582 - val_loss: 11.8212 - val_acc: 0.9461\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 11.27620\n",
      "Epoch 25/50\n",
      "160000/160000 [==============================] - 1882s 12ms/step - loss: 7.4088 - acc: 0.9585 - val_loss: 11.4296 - val_acc: 0.9462\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 11.27620\n",
      "Epoch 26/50\n",
      "160000/160000 [==============================] - 1726s 11ms/step - loss: 7.3438 - acc: 0.9587 - val_loss: 11.2938 - val_acc: 0.9466\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 11.27620\n",
      "Epoch 27/50\n",
      "160000/160000 [==============================] - 1886s 12ms/step - loss: 7.2759 - acc: 0.9589 - val_loss: 11.3651 - val_acc: 0.9469\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 11.27620\n",
      "Epoch 28/50\n",
      "160000/160000 [==============================] - 1709s 11ms/step - loss: 7.2122 - acc: 0.9591 - val_loss: 11.0956 - val_acc: 0.9475\n",
      "\n",
      "Epoch 00028: val_loss improved from 11.27620 to 11.09555, saving model to ./result/09_no_contrast_64_Attn_notNaive_hybridTest/best_weights.h5\n",
      "Epoch 29/50\n",
      "160000/160000 [==============================] - 1880s 12ms/step - loss: 7.1456 - acc: 0.9594 - val_loss: 11.5945 - val_acc: 0.9467\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 11.09555\n",
      "Epoch 30/50\n",
      "160000/160000 [==============================] - 1758s 11ms/step - loss: 7.0858 - acc: 0.9595 - val_loss: 11.0256 - val_acc: 0.9472\n",
      "\n",
      "Epoch 00030: val_loss improved from 11.09555 to 11.02560, saving model to ./result/09_no_contrast_64_Attn_notNaive_hybridTest/best_weights.h5\n",
      "Epoch 31/50\n",
      "160000/160000 [==============================] - 1877s 12ms/step - loss: 7.0450 - acc: 0.9597 - val_loss: 11.6850 - val_acc: 0.9470\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 11.02560\n",
      "Epoch 32/50\n",
      "160000/160000 [==============================] - 1736s 11ms/step - loss: 6.9774 - acc: 0.9599 - val_loss: 11.7705 - val_acc: 0.9469\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 11.02560\n",
      "Epoch 33/50\n",
      "160000/160000 [==============================] - 1884s 12ms/step - loss: 6.9284 - acc: 0.9601 - val_loss: 11.3779 - val_acc: 0.9471\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 11.02560\n",
      "Epoch 34/50\n",
      "160000/160000 [==============================] - 1742s 11ms/step - loss: 6.8776 - acc: 0.9602 - val_loss: 11.6770 - val_acc: 0.9478\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 11.02560\n",
      "Epoch 35/50\n",
      "160000/160000 [==============================] - 1870s 12ms/step - loss: 6.8195 - acc: 0.9604 - val_loss: 11.4443 - val_acc: 0.9466\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 11.02560\n",
      "Epoch 36/50\n",
      "160000/160000 [==============================] - 1741s 11ms/step - loss: 6.7891 - acc: 0.9606 - val_loss: 11.1478 - val_acc: 0.9468\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 11.02560\n",
      "Epoch 37/50\n",
      "160000/160000 [==============================] - 1913s 12ms/step - loss: 6.7272 - acc: 0.9608 - val_loss: 10.9131 - val_acc: 0.9473\n",
      "\n",
      "Epoch 00037: val_loss improved from 11.02560 to 10.91309, saving model to ./result/09_no_contrast_64_Attn_notNaive_hybridTest/best_weights.h5\n",
      "Epoch 38/50\n",
      "160000/160000 [==============================] - 1772s 11ms/step - loss: 6.6869 - acc: 0.9609 - val_loss: 12.2954 - val_acc: 0.9471\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 10.91309\n",
      "Epoch 39/50\n",
      "160000/160000 [==============================] - 2020s 13ms/step - loss: 6.6465 - acc: 0.9611 - val_loss: 10.8435 - val_acc: 0.9475\n",
      "\n",
      "Epoch 00039: val_loss improved from 10.91309 to 10.84354, saving model to ./result/09_no_contrast_64_Attn_notNaive_hybridTest/best_weights.h5\n",
      "Epoch 40/50\n",
      "160000/160000 [==============================] - 1881s 12ms/step - loss: 6.6081 - acc: 0.9612 - val_loss: 11.6497 - val_acc: 0.9470\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 10.84354\n",
      "Epoch 41/50\n",
      "160000/160000 [==============================] - 2039s 13ms/step - loss: 6.5673 - acc: 0.9613 - val_loss: 11.4841 - val_acc: 0.9475\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 10.84354\n",
      "Epoch 42/50\n",
      "160000/160000 [==============================] - 1835s 11ms/step - loss: 6.5301 - acc: 0.9615 - val_loss: 10.9457 - val_acc: 0.9482\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 10.84354\n",
      "Epoch 43/50\n",
      "160000/160000 [==============================] - 1982s 12ms/step - loss: 6.4820 - acc: 0.9616 - val_loss: 11.6027 - val_acc: 0.9482\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 10.84354\n",
      "Epoch 44/50\n",
      "160000/160000 [==============================] - 1855s 12ms/step - loss: 6.4505 - acc: 0.9617 - val_loss: 11.7086 - val_acc: 0.9484\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 10.84354\n",
      "Epoch 45/50\n",
      "160000/160000 [==============================] - 1982s 12ms/step - loss: 6.4205 - acc: 0.9619 - val_loss: 11.0780 - val_acc: 0.9472\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 10.84354\n",
      "Epoch 46/50\n",
      "160000/160000 [==============================] - 1828s 11ms/step - loss: 6.3845 - acc: 0.9620 - val_loss: 11.4784 - val_acc: 0.9479\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 10.84354\n",
      "Epoch 47/50\n",
      "160000/160000 [==============================] - 1976s 12ms/step - loss: 6.3444 - acc: 0.9621 - val_loss: 11.9028 - val_acc: 0.9480\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 10.84354\n",
      "Epoch 48/50\n",
      "160000/160000 [==============================] - 1834s 11ms/step - loss: 6.3121 - acc: 0.9622 - val_loss: 11.7485 - val_acc: 0.9469\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 10.84354\n",
      "Epoch 49/50\n",
      "160000/160000 [==============================] - 2001s 13ms/step - loss: 6.2786 - acc: 0.9623 - val_loss: 11.9743 - val_acc: 0.9484\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 10.84354\n",
      "Epoch 50/50\n",
      "160000/160000 [==============================] - 1829s 11ms/step - loss: 6.2547 - acc: 0.9624 - val_loss: 12.1304 - val_acc: 0.9480\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 10.84354\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    history = model.fit(patches_imgs_train, patches_masks_train, epochs=num_epochs, batch_size=batch_size, verbose=1, shuffle=True, validation_split=0.2, callbacks=[checkpointer])\n",
    "    model.save_weights('./'+save_folder+'/'+name_experiment +'/last_weights.h5', overwrite=False)\n",
    "except KeyboardInterrupt:\n",
    "    model.save_weights('./'+save_folder+'/'+name_experiment +'/last_weights.h5', overwrite=False)\n",
    "    print('Keyboard Interrupt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(patches_imgs_train, patches_masks_train, epochs=num_epochs, batch_size=batch_size, verbose=1, shuffle=True, validation_split=0.2, callbacks=[checkpointer])\n",
    "model.save_weights('./'+save_folder+'/'+name_experiment +'/last_weights.h5', overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Keras provides a set of functions called callbacks: \n",
    "you can think of callbacks as events that will be triggered at certain training states. \n",
    "The callback we need for checkpointing is the ModelCheckpoint \n",
    "which provides all the features we need according to the checkpointing strategy we adopted in our example\n",
    "\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='./'+save_folder+'/'+name_experiment+'/best_weights.h5', verbose=1, monitor='val_loss', mode='auto', save_best_only=True) #save at each epoch if the validation decreased\n",
    "\n",
    "print('[training session] before mask unet func patch mask shape : ',patches_masks_train.shape)\n",
    "patches_masks_train = masks_Unet(patches_masks_train)  #reduce memory consumption\n",
    "print('[training session] after mask unet func patch mask shape : ',patches_masks_train.shape)\n",
    "\n",
    "#print(len(x_train))\n",
    "history = model.fit_generator(data_gen.flow(patches_imgs_train,patches_masks_train,batch_size=batch_size,subset='training'), \n",
    "                              steps_per_epoch=len(patches_imgs_train)/batch_size,epochs=num_epochs, verbose=1,shuffle=True, callbacks=[checkpointer],\n",
    "                             validation_data = data_gen.flow(patches_imgs_train,patches_masks_train,batch_size=batch_size,subset='validation'),\n",
    "                             validation_steps = 10)\n",
    "model.save_weights('./'+save_folder+'/'+name_experiment +'/last_weights.h5', overwrite=True) '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'val_loss', 'acc', 'val_acc'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already exist the folder in this path : ./result/09_no_contrast_64_Attn_notNaive_hybridTest/train_history\n",
      "already exist the folder in this path : ./result/09_no_contrast_64_Attn_notNaive_hybridTest/train_history\n",
      "already exist the folder in this path : ./result/09_no_contrast_64_Attn_notNaive_hybridTest/train_history\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(history.history.keys())\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss= history.history['val_loss']\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "epochs = range(1,len(acc) +1)\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label = \"Training loss\")\n",
    "plt.plot(epochs, val_loss, 'b', label = \"Validation loss\")\n",
    "plt.title(\"Training and Validation loss\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.savefig('./'+save_folder+'/'+name_experiment+\"/training_loss_result.png\")\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, acc, 'ro', label = \"Training accuracy\")\n",
    "plt.plot(epochs, val_acc, 'r', label = \"Validation accuracy\")\n",
    "plt.title(\"Training and Validation dice coef\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel('acc')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.savefig('./'+save_folder+'/'+name_experiment+\"/training_acc_result.png\")\n",
    "plt.show()\n",
    "\n",
    "import pandas as pd\n",
    "file_path = './'+save_folder+'/'+name_experiment + '/' + 'train_history'\n",
    "\n",
    "def save_history_txt_csv(history, file_path, file_name):\n",
    "    if os.path.isdir(file_path) == False:\n",
    "        os.mkdir(file_path)\n",
    "    else:\n",
    "        print('already exist the folder in this path : {}'.format(file_path))\n",
    "    \n",
    "    hist_df = pd.DataFrame(history) \n",
    "\n",
    "    # save to json:  \n",
    "    hist_json_file = file_path + '/' + file_name +'.json' \n",
    "    with open(hist_json_file, mode='w') as f:\n",
    "        hist_df.to_json(f)\n",
    "\n",
    "    # or save to csv: \n",
    "    hist_csv_file = file_path + '/' + file_name + '.csv'\n",
    "    with open(hist_csv_file, mode='w') as f:\n",
    "        hist_df.to_csv(f)\n",
    "        \n",
    "\n",
    "save_history_txt_csv(loss, file_path, 'train_loss')\n",
    "save_history_txt_csv(val_loss, file_path, 'val_loss')\n",
    "save_history_txt_csv(acc, file_path, 'train_acc')\n",
    "save_history_txt_csv(val_acc, file_path, 'val_acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "file_path = './'+save_folder+'/'+name_experiment + '/' + 'train_history'\n",
    "\n",
    "def save_history_txt_csv(history, file_path, file_name):\n",
    "    if os.path.isdir(file_path) == False:\n",
    "        os.mkdir(file_path)\n",
    "    else:\n",
    "        print('already exist the folder in this path : {}'.format(file_path))\n",
    "    \n",
    "    hist_df = pd.DataFrame(history) \n",
    "\n",
    "    # save to json:  \n",
    "    hist_json_file = file_path + '/' + file_name +'.json' \n",
    "    with open(hist_json_file, mode='w') as f:\n",
    "        hist_df.to_json(f)\n",
    "\n",
    "    # or save to csv: \n",
    "    hist_csv_file = file_path + '/' + file_name + '.csv'\n",
    "    with open(hist_csv_file, mode='w') as f:\n",
    "        hist_df.to_csv(f)\n",
    "        \n",
    "\n",
    "save_history_txt_csv(loss, file_path, 'train_loss')\n",
    "save_history_txt_csv(val_loss, file_path, 'val_loss')\n",
    "save_history_txt_csv(acc, file_path, 'train_acc')\n",
    "save_history_txt_csv(val_acc, file_path, 'val_acc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(history.history.keys())\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss= history.history['val_loss']\n",
    "acc = history.history['dice_coef']\n",
    "val_acc = history.history['val_dice_coef']\n",
    "\n",
    "epochs = range(1,len(acc) +1)\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label = \"Training loss\")\n",
    "plt.plot(epochs, val_loss, 'b', label = \"Validation loss\")\n",
    "plt.title(\"Training and Validation loss\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.savefig('./'+save_folder+'/'+name_experiment+\"/training_loss_result.png\")\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, acc, 'ro', label = \"Training dice coef\")\n",
    "plt.plot(epochs, val_acc, 'r', label = \"Validation dice coef\")\n",
    "plt.title(\"Training and Validation dice coef\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel('acc')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.savefig('./'+save_folder+'/'+name_experiment+\"/training_acc_result.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
