{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/bono/.pyenv/versions/3.5.5/envs/gpuTest/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/bono/.pyenv/versions/3.5.5/envs/gpuTest/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/bono/.pyenv/versions/3.5.5/envs/gpuTest/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/bono/.pyenv/versions/3.5.5/envs/gpuTest/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:522: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/bono/.pyenv/versions/3.5.5/envs/gpuTest/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/bono/.pyenv/versions/3.5.5/envs/gpuTest/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/job:localhost/replica:0/task:0/device:GPU:0']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import configparser\n",
    "import shutil\n",
    "import cv2\n",
    "\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras import backend as K\n",
    "from keras.utils.vis_utils import plot_model as plotn\n",
    "from keras.optimizers import SGD\n",
    "from keras import models\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, './lib_keras/')\n",
    "from help_functions import *\n",
    "\n",
    "from lib_keras.model_lib import *\n",
    "#function to obtain data for training/testing (validation)\n",
    "from temp_extract_patches import temp_get_data_training\n",
    "import random\n",
    "from tensorflow import set_random_seed\n",
    "\n",
    "print(K.tensorflow_backend._get_available_gpus())\n",
    "SEED = 5\n",
    "\n",
    "np.random.seed(SEED)\n",
    "os.environ['PYTHONHASHSEED']=str(SEED)\n",
    "set_random_seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.RawConfigParser()\n",
    "config.read('configuration.txt')\n",
    "#patch to the datasets\n",
    "path_data = config.get('data paths', 'path_local')\n",
    "#Experiment name\n",
    "save_folder = config.get('experiment name','result_save_path')\n",
    "name_experiment = config.get('experiment name', 'name')\n",
    "\n",
    "#training settings\n",
    "num_epochs = int(config.get('training settings', 'num_epochs'))\n",
    "batch_size = int(config.get('training settings', 'batch_size'))\n",
    "\n",
    "mapping = OrderedDict()\n",
    "mapping[(0,0,0)] = 0\n",
    "mapping[(255,0,0)] = 1\n",
    "mapping[(0,0,255)] = 2\n",
    "\n",
    "if os.path.isdir('./'+save_folder+'/'+name_experiment+'/') == False:\n",
    "    os.mkdir('./'+save_folder+'/'+name_experiment+'/')\n",
    "else:\n",
    "    print('already exist the folder in this path : {}'.format('./'+save_folder+'/'+name_experiment+'/'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already exist the folder in this path : result/20_0621_fine_segmentor_180_Data_focal_gamma_07\n",
      "number of subimages :  240000\n",
      "[DEBUG] shape of train_imgs_original :  (180, 3, 500, 760)\n",
      "[DEBUG] shape of train_imgs_label :  (180, 3, 500, 760)\n",
      "\n",
      "\n",
      "[get_data_training] preprocessed image shape :  (180, 1, 500, 760)\n",
      "\n",
      "[get_data_training] preprocessed mask shape :  (180, 3, 500, 760)\n",
      "mask maximum val :  255.0\n",
      "[get_data_training] preprocessed2 image shape :  (180, 1, 500, 760)\n",
      "\n",
      "\n",
      "[padding] pad h size : 12\t pad w size : 8\n",
      "\n",
      "\n",
      "[padding] imgs shape : (180, 500, 760, 1)\t labels shape : (180, 500, 760, 3)\n",
      "\n",
      "\n",
      "[padding] pad imgs shape : (180, 512, 768, 1)\t pad labels shape : (180, 512, 768, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "extract patches:   0%|          | 0/180 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[final pad] imgs shape : (180, 512, 768, 1)\t labels shape : (180, 512, 768, 3)\n",
      "\n",
      "\n",
      "[final transpose] imgs shape : (180, 1, 512, 768)\t labels shape : (180, 3, 512, 768)\n",
      "\n",
      "\n",
      "[get_data_training] train images/masks shape : (180, 1, 512, 768)\n",
      "[get_data_training] train images range (min-max) [0.0 , 255.0] \n",
      "[get_data_training] train masks are within 0-1\n",
      "\n",
      "\n",
      "\n",
      "[extract random] num of class :  3\n",
      "[extract random] full image shape : (180, 1, 512, 768)\n",
      "[extract random] full masks shape : (180, 3, 512, 768)\n",
      "[extract random] patches shape : (240000, 1, 64, 64)\n",
      "[extract random] patches masks shape : (240000, 3, 64, 64)\n",
      "[extract random] patches per full image : 1333\n",
      "aug patch shape : (64, 64, 1) aug mask shape : (64, 64, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "extract patches: 100%|██████████| 180/180 [11:54<00:00,  3.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[After patch] mask shape :  (240000, 3, 64, 64)\n",
      "\n",
      "\n",
      "[get_data_training] train PATCHES images/masks shape : (240000, 1, 64, 64)\n",
      "[get_data_training] train PATCHES images range (min-max): 0.0 - 1.0\n",
      "[get_data_training] patches_imgs_train : (240000, 1, 64, 64)\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "patches_imgs_train, patches_masks_train, class_freq_tabel = temp_get_data_training(\n",
    "    train_imgs_original = path_data + config.get('data paths', 'train_imgs_original'),\n",
    "    train_groudTruth = path_data + config.get('data paths', 'train_groundTruth'),  #masks\n",
    "    patch_height = int(config.get('data attributes', 'patch_height')),\n",
    "    patch_width = int(config.get('data attributes', 'patch_width')),\n",
    "    num_subimgs = int(config.get('training settings', 'num_subimgs')),\n",
    "    label_mapping = mapping,\n",
    "    inside_FOV = config.getboolean('training settings', 'inside_FOV'), #select the patches only inside the FOV  (default == True)\n",
    "    save_path = save_folder+'/'+name_experiment\n",
    ")\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_0</th>\n",
       "      <th>class_1</th>\n",
       "      <th>class_2</th>\n",
       "      <th>frequency_0</th>\n",
       "      <th>frequency_1</th>\n",
       "      <th>frequency_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5364470</td>\n",
       "      <td>70627</td>\n",
       "      <td>70627</td>\n",
       "      <td>0.974344</td>\n",
       "      <td>0.0128279</td>\n",
       "      <td>0.0128279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5274082</td>\n",
       "      <td>165015</td>\n",
       "      <td>165015</td>\n",
       "      <td>0.941109</td>\n",
       "      <td>0.0294453</td>\n",
       "      <td>0.0294453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5260732</td>\n",
       "      <td>177547</td>\n",
       "      <td>177547</td>\n",
       "      <td>0.936769</td>\n",
       "      <td>0.0316155</td>\n",
       "      <td>0.0316155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5271121</td>\n",
       "      <td>174549</td>\n",
       "      <td>174549</td>\n",
       "      <td>0.937885</td>\n",
       "      <td>0.0310573</td>\n",
       "      <td>0.0310573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5270079</td>\n",
       "      <td>168462</td>\n",
       "      <td>168462</td>\n",
       "      <td>0.93991</td>\n",
       "      <td>0.0300449</td>\n",
       "      <td>0.0300449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5248362</td>\n",
       "      <td>192492</td>\n",
       "      <td>192492</td>\n",
       "      <td>0.93166</td>\n",
       "      <td>0.0341701</td>\n",
       "      <td>0.0341701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5273102</td>\n",
       "      <td>166100</td>\n",
       "      <td>166100</td>\n",
       "      <td>0.940735</td>\n",
       "      <td>0.0296327</td>\n",
       "      <td>0.0296327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5238733</td>\n",
       "      <td>199414</td>\n",
       "      <td>199414</td>\n",
       "      <td>0.929255</td>\n",
       "      <td>0.0353724</td>\n",
       "      <td>0.0353724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5277064</td>\n",
       "      <td>169227</td>\n",
       "      <td>169227</td>\n",
       "      <td>0.939729</td>\n",
       "      <td>0.0301356</td>\n",
       "      <td>0.0301356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5281500</td>\n",
       "      <td>163447</td>\n",
       "      <td>163447</td>\n",
       "      <td>0.941713</td>\n",
       "      <td>0.0291433</td>\n",
       "      <td>0.0291433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5276127</td>\n",
       "      <td>162081</td>\n",
       "      <td>162081</td>\n",
       "      <td>0.942117</td>\n",
       "      <td>0.0289415</td>\n",
       "      <td>0.0289415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5244904</td>\n",
       "      <td>182227</td>\n",
       "      <td>182227</td>\n",
       "      <td>0.935028</td>\n",
       "      <td>0.0324862</td>\n",
       "      <td>0.0324862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5243452</td>\n",
       "      <td>176815</td>\n",
       "      <td>176815</td>\n",
       "      <td>0.936819</td>\n",
       "      <td>0.0315906</td>\n",
       "      <td>0.0315906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5265620</td>\n",
       "      <td>174779</td>\n",
       "      <td>174779</td>\n",
       "      <td>0.937748</td>\n",
       "      <td>0.0311262</td>\n",
       "      <td>0.0311262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5278627</td>\n",
       "      <td>164379</td>\n",
       "      <td>164379</td>\n",
       "      <td>0.941371</td>\n",
       "      <td>0.0293147</td>\n",
       "      <td>0.0293147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5283948</td>\n",
       "      <td>157065</td>\n",
       "      <td>157065</td>\n",
       "      <td>0.943886</td>\n",
       "      <td>0.028057</td>\n",
       "      <td>0.028057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5257280</td>\n",
       "      <td>179169</td>\n",
       "      <td>179169</td>\n",
       "      <td>0.936189</td>\n",
       "      <td>0.0319055</td>\n",
       "      <td>0.0319055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5268594</td>\n",
       "      <td>175915</td>\n",
       "      <td>175915</td>\n",
       "      <td>0.937402</td>\n",
       "      <td>0.0312992</td>\n",
       "      <td>0.0312992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5268221</td>\n",
       "      <td>166304</td>\n",
       "      <td>166304</td>\n",
       "      <td>0.940615</td>\n",
       "      <td>0.0296927</td>\n",
       "      <td>0.0296927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5230757</td>\n",
       "      <td>210193</td>\n",
       "      <td>210193</td>\n",
       "      <td>0.92561</td>\n",
       "      <td>0.0371948</td>\n",
       "      <td>0.0371948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5283686</td>\n",
       "      <td>154000</td>\n",
       "      <td>154000</td>\n",
       "      <td>0.944918</td>\n",
       "      <td>0.0275409</td>\n",
       "      <td>0.0275409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5265447</td>\n",
       "      <td>167425</td>\n",
       "      <td>167425</td>\n",
       "      <td>0.940209</td>\n",
       "      <td>0.0298957</td>\n",
       "      <td>0.0298957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5259283</td>\n",
       "      <td>169515</td>\n",
       "      <td>169515</td>\n",
       "      <td>0.939441</td>\n",
       "      <td>0.0302797</td>\n",
       "      <td>0.0302797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5249014</td>\n",
       "      <td>181365</td>\n",
       "      <td>181365</td>\n",
       "      <td>0.935362</td>\n",
       "      <td>0.0323188</td>\n",
       "      <td>0.0323188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>5278788</td>\n",
       "      <td>161072</td>\n",
       "      <td>161072</td>\n",
       "      <td>0.942484</td>\n",
       "      <td>0.0287581</td>\n",
       "      <td>0.0287581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5266439</td>\n",
       "      <td>162592</td>\n",
       "      <td>162592</td>\n",
       "      <td>0.941844</td>\n",
       "      <td>0.0290778</td>\n",
       "      <td>0.0290778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5294156</td>\n",
       "      <td>146171</td>\n",
       "      <td>146171</td>\n",
       "      <td>0.94767</td>\n",
       "      <td>0.0261651</td>\n",
       "      <td>0.0261651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5278680</td>\n",
       "      <td>153790</td>\n",
       "      <td>153790</td>\n",
       "      <td>0.94494</td>\n",
       "      <td>0.02753</td>\n",
       "      <td>0.02753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5284616</td>\n",
       "      <td>153086</td>\n",
       "      <td>153086</td>\n",
       "      <td>0.945236</td>\n",
       "      <td>0.0273818</td>\n",
       "      <td>0.0273818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5260663</td>\n",
       "      <td>168094</td>\n",
       "      <td>168094</td>\n",
       "      <td>0.939933</td>\n",
       "      <td>0.0300337</td>\n",
       "      <td>0.0300337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>5266528</td>\n",
       "      <td>165334</td>\n",
       "      <td>165334</td>\n",
       "      <td>0.940923</td>\n",
       "      <td>0.0295387</td>\n",
       "      <td>0.0295387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>5240212</td>\n",
       "      <td>199735</td>\n",
       "      <td>199735</td>\n",
       "      <td>0.929168</td>\n",
       "      <td>0.035416</td>\n",
       "      <td>0.035416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>5254955</td>\n",
       "      <td>186840</td>\n",
       "      <td>186840</td>\n",
       "      <td>0.933611</td>\n",
       "      <td>0.0331945</td>\n",
       "      <td>0.0331945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>5236558</td>\n",
       "      <td>208418</td>\n",
       "      <td>208418</td>\n",
       "      <td>0.926268</td>\n",
       "      <td>0.036866</td>\n",
       "      <td>0.036866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>5253916</td>\n",
       "      <td>180762</td>\n",
       "      <td>180762</td>\n",
       "      <td>0.93562</td>\n",
       "      <td>0.0321902</td>\n",
       "      <td>0.0321902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>5247078</td>\n",
       "      <td>193995</td>\n",
       "      <td>193995</td>\n",
       "      <td>0.931147</td>\n",
       "      <td>0.0344264</td>\n",
       "      <td>0.0344264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>5215628</td>\n",
       "      <td>209666</td>\n",
       "      <td>209666</td>\n",
       "      <td>0.925584</td>\n",
       "      <td>0.0372081</td>\n",
       "      <td>0.0372081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>5225563</td>\n",
       "      <td>181721</td>\n",
       "      <td>181721</td>\n",
       "      <td>0.934972</td>\n",
       "      <td>0.032514</td>\n",
       "      <td>0.032514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>5254440</td>\n",
       "      <td>187911</td>\n",
       "      <td>187911</td>\n",
       "      <td>0.93325</td>\n",
       "      <td>0.0333752</td>\n",
       "      <td>0.0333752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>5234929</td>\n",
       "      <td>212267</td>\n",
       "      <td>212267</td>\n",
       "      <td>0.924987</td>\n",
       "      <td>0.0375066</td>\n",
       "      <td>0.0375066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>5245300</td>\n",
       "      <td>201668</td>\n",
       "      <td>201668</td>\n",
       "      <td>0.928596</td>\n",
       "      <td>0.0357021</td>\n",
       "      <td>0.0357021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>5263754</td>\n",
       "      <td>172030</td>\n",
       "      <td>172030</td>\n",
       "      <td>0.938646</td>\n",
       "      <td>0.0306768</td>\n",
       "      <td>0.0306768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>5228059</td>\n",
       "      <td>214318</td>\n",
       "      <td>214318</td>\n",
       "      <td>0.924225</td>\n",
       "      <td>0.0378875</td>\n",
       "      <td>0.0378875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>5262403</td>\n",
       "      <td>185879</td>\n",
       "      <td>185879</td>\n",
       "      <td>0.934017</td>\n",
       "      <td>0.0329914</td>\n",
       "      <td>0.0329914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>5276957</td>\n",
       "      <td>168182</td>\n",
       "      <td>168182</td>\n",
       "      <td>0.940078</td>\n",
       "      <td>0.0299612</td>\n",
       "      <td>0.0299612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>5264149</td>\n",
       "      <td>185606</td>\n",
       "      <td>185606</td>\n",
       "      <td>0.934128</td>\n",
       "      <td>0.032936</td>\n",
       "      <td>0.032936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>5276336</td>\n",
       "      <td>167319</td>\n",
       "      <td>167319</td>\n",
       "      <td>0.94036</td>\n",
       "      <td>0.02982</td>\n",
       "      <td>0.02982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>5268822</td>\n",
       "      <td>173317</td>\n",
       "      <td>173317</td>\n",
       "      <td>0.938271</td>\n",
       "      <td>0.0308643</td>\n",
       "      <td>0.0308643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>5258022</td>\n",
       "      <td>181534</td>\n",
       "      <td>181534</td>\n",
       "      <td>0.93541</td>\n",
       "      <td>0.0322952</td>\n",
       "      <td>0.0322952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>5276056</td>\n",
       "      <td>167356</td>\n",
       "      <td>167356</td>\n",
       "      <td>0.940345</td>\n",
       "      <td>0.0298276</td>\n",
       "      <td>0.0298276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>5240228</td>\n",
       "      <td>201263</td>\n",
       "      <td>201263</td>\n",
       "      <td>0.928665</td>\n",
       "      <td>0.0356675</td>\n",
       "      <td>0.0356675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>5263556</td>\n",
       "      <td>183799</td>\n",
       "      <td>183799</td>\n",
       "      <td>0.934721</td>\n",
       "      <td>0.0326397</td>\n",
       "      <td>0.0326397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>5259234</td>\n",
       "      <td>186054</td>\n",
       "      <td>186054</td>\n",
       "      <td>0.933922</td>\n",
       "      <td>0.033039</td>\n",
       "      <td>0.033039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>5282044</td>\n",
       "      <td>166940</td>\n",
       "      <td>166940</td>\n",
       "      <td>0.940548</td>\n",
       "      <td>0.0297262</td>\n",
       "      <td>0.0297262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>5247316</td>\n",
       "      <td>194499</td>\n",
       "      <td>194499</td>\n",
       "      <td>0.930984</td>\n",
       "      <td>0.0345082</td>\n",
       "      <td>0.0345082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>5251094</td>\n",
       "      <td>193762</td>\n",
       "      <td>193762</td>\n",
       "      <td>0.931273</td>\n",
       "      <td>0.0343634</td>\n",
       "      <td>0.0343634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>5260189</td>\n",
       "      <td>177301</td>\n",
       "      <td>177301</td>\n",
       "      <td>0.936845</td>\n",
       "      <td>0.0315775</td>\n",
       "      <td>0.0315775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>5263789</td>\n",
       "      <td>171197</td>\n",
       "      <td>171197</td>\n",
       "      <td>0.938926</td>\n",
       "      <td>0.0305372</td>\n",
       "      <td>0.0305372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>5303096</td>\n",
       "      <td>138980</td>\n",
       "      <td>138980</td>\n",
       "      <td>0.950196</td>\n",
       "      <td>0.0249021</td>\n",
       "      <td>0.0249021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>5267574</td>\n",
       "      <td>178067</td>\n",
       "      <td>178067</td>\n",
       "      <td>0.936673</td>\n",
       "      <td>0.0316636</td>\n",
       "      <td>0.0316636</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     class_0 class_1 class_2 frequency_0 frequency_1 frequency_2\n",
       "0    5364470   70627   70627    0.974344   0.0128279   0.0128279\n",
       "1    5274082  165015  165015    0.941109   0.0294453   0.0294453\n",
       "2    5260732  177547  177547    0.936769   0.0316155   0.0316155\n",
       "3    5271121  174549  174549    0.937885   0.0310573   0.0310573\n",
       "4    5270079  168462  168462     0.93991   0.0300449   0.0300449\n",
       "5    5248362  192492  192492     0.93166   0.0341701   0.0341701\n",
       "6    5273102  166100  166100    0.940735   0.0296327   0.0296327\n",
       "7    5238733  199414  199414    0.929255   0.0353724   0.0353724\n",
       "8    5277064  169227  169227    0.939729   0.0301356   0.0301356\n",
       "9    5281500  163447  163447    0.941713   0.0291433   0.0291433\n",
       "10   5276127  162081  162081    0.942117   0.0289415   0.0289415\n",
       "11   5244904  182227  182227    0.935028   0.0324862   0.0324862\n",
       "12   5243452  176815  176815    0.936819   0.0315906   0.0315906\n",
       "13   5265620  174779  174779    0.937748   0.0311262   0.0311262\n",
       "14   5278627  164379  164379    0.941371   0.0293147   0.0293147\n",
       "15   5283948  157065  157065    0.943886    0.028057    0.028057\n",
       "16   5257280  179169  179169    0.936189   0.0319055   0.0319055\n",
       "17   5268594  175915  175915    0.937402   0.0312992   0.0312992\n",
       "18   5268221  166304  166304    0.940615   0.0296927   0.0296927\n",
       "19   5230757  210193  210193     0.92561   0.0371948   0.0371948\n",
       "20   5283686  154000  154000    0.944918   0.0275409   0.0275409\n",
       "21   5265447  167425  167425    0.940209   0.0298957   0.0298957\n",
       "22   5259283  169515  169515    0.939441   0.0302797   0.0302797\n",
       "23   5249014  181365  181365    0.935362   0.0323188   0.0323188\n",
       "24   5278788  161072  161072    0.942484   0.0287581   0.0287581\n",
       "25   5266439  162592  162592    0.941844   0.0290778   0.0290778\n",
       "26   5294156  146171  146171     0.94767   0.0261651   0.0261651\n",
       "27   5278680  153790  153790     0.94494     0.02753     0.02753\n",
       "28   5284616  153086  153086    0.945236   0.0273818   0.0273818\n",
       "29   5260663  168094  168094    0.939933   0.0300337   0.0300337\n",
       "..       ...     ...     ...         ...         ...         ...\n",
       "150  5266528  165334  165334    0.940923   0.0295387   0.0295387\n",
       "151  5240212  199735  199735    0.929168    0.035416    0.035416\n",
       "152  5254955  186840  186840    0.933611   0.0331945   0.0331945\n",
       "153  5236558  208418  208418    0.926268    0.036866    0.036866\n",
       "154  5253916  180762  180762     0.93562   0.0321902   0.0321902\n",
       "155  5247078  193995  193995    0.931147   0.0344264   0.0344264\n",
       "156  5215628  209666  209666    0.925584   0.0372081   0.0372081\n",
       "157  5225563  181721  181721    0.934972    0.032514    0.032514\n",
       "158  5254440  187911  187911     0.93325   0.0333752   0.0333752\n",
       "159  5234929  212267  212267    0.924987   0.0375066   0.0375066\n",
       "160  5245300  201668  201668    0.928596   0.0357021   0.0357021\n",
       "161  5263754  172030  172030    0.938646   0.0306768   0.0306768\n",
       "162  5228059  214318  214318    0.924225   0.0378875   0.0378875\n",
       "163  5262403  185879  185879    0.934017   0.0329914   0.0329914\n",
       "164  5276957  168182  168182    0.940078   0.0299612   0.0299612\n",
       "165  5264149  185606  185606    0.934128    0.032936    0.032936\n",
       "166  5276336  167319  167319     0.94036     0.02982     0.02982\n",
       "167  5268822  173317  173317    0.938271   0.0308643   0.0308643\n",
       "168  5258022  181534  181534     0.93541   0.0322952   0.0322952\n",
       "169  5276056  167356  167356    0.940345   0.0298276   0.0298276\n",
       "170  5240228  201263  201263    0.928665   0.0356675   0.0356675\n",
       "171  5263556  183799  183799    0.934721   0.0326397   0.0326397\n",
       "172  5259234  186054  186054    0.933922    0.033039    0.033039\n",
       "173  5282044  166940  166940    0.940548   0.0297262   0.0297262\n",
       "174  5247316  194499  194499    0.930984   0.0345082   0.0345082\n",
       "175  5251094  193762  193762    0.931273   0.0343634   0.0343634\n",
       "176  5260189  177301  177301    0.936845   0.0315775   0.0315775\n",
       "177  5263789  171197  171197    0.938926   0.0305372   0.0305372\n",
       "178  5303096  138980  138980    0.950196   0.0249021   0.0249021\n",
       "179  5267574  178067  178067    0.936673   0.0316636   0.0316636\n",
       "\n",
       "[180 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_freq_tabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "average_class_freq = pd.DataFrame(columns=['avg_class_00', 'avg_class_01', 'avg_class_02', 'avg_freq_00','avg_freq_01','avg_freq_02'])\n",
    "\n",
    "average_class_freq.loc[0,'avg_class_00'] = class_freq_tabel['class_0'].mean()\n",
    "average_class_freq.loc[0,'avg_class_01'] = class_freq_tabel['class_1'].mean()\n",
    "average_class_freq.loc[0,'avg_class_02'] = class_freq_tabel['class_2'].mean()\n",
    "\n",
    "\n",
    "average_class_freq.loc[0,'avg_freq_00'] = class_freq_tabel['frequency_0'].mean()\n",
    "average_class_freq.loc[0,'avg_freq_01'] = class_freq_tabel['frequency_1'].mean()\n",
    "average_class_freq.loc[0,'avg_freq_02'] = class_freq_tabel['frequency_2'].mean()\n",
    "\n",
    "class_freq_tabel.to_csv('./'+save_folder+'/'+name_experiment+'/'+'all_class_imbalance.csv', encoding='utf-8')\n",
    "average_class_freq.to_csv('./'+save_folder+'/'+name_experiment+'/'+'avg_class_imbalance.csv', encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240000, 1, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "patches_masks_train_class00 = patches_masks_train[:,0,:,:] \n",
    "patches_masks_train_class00 = np.expand_dims(patches_masks_train_class00,1)\n",
    "patches_masks_train_class01 = patches_masks_train[:,1,:,:] \n",
    "patches_masks_train_class01 = np.expand_dims(patches_masks_train_class01,1)\n",
    "patches_masks_train_class02 = patches_masks_train[:,2,:,:] \n",
    "patches_masks_train_class02 = np.expand_dims(patches_masks_train_class02,1)\n",
    "\n",
    "\n",
    "\n",
    "print(np.shape(patches_masks_train_class00))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=320x576 at 0x7FB1045DE128>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_sample = min(patches_imgs_train.shape[0],40)\n",
    "visualize(group_images(patches_imgs_train[0:N_sample,:,:,:],5),'./'+save_folder+'/'+name_experiment+'/'+\"sample_input_imgs\")#.show()\n",
    "visualize(group_images(patches_masks_train_class00[0:N_sample,:,:,:],5),'./'+save_folder+'/'+name_experiment+'/'+\"sample_input_masks_claass0\")#.show()\n",
    "visualize(group_images(patches_masks_train_class01[0:N_sample,:,:,:],5),'./'+save_folder+'/'+name_experiment+'/'+\"sample_input_masks_claass1\")#.show()\n",
    "visualize(group_images(patches_masks_train_class02[0:N_sample,:,:,:],5),'./'+save_folder+'/'+name_experiment+'/'+\"sample_input_masks_claass2\")#.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape :  (240000, 1, 64, 64)\n",
      "n_ch : 1 patch_h : 64 patch_w : 64\n",
      "input shape :  (None, 1, 64, 64)\n",
      "\n",
      "gating shape : (None, 128, 4, 4), conv4 shape : (None, 64, 8, 8)\n",
      "shape x,g  (None, 64, 8, 8) (None, 128, 4, 4)\n",
      "inter shape :   128\n",
      "stride x : 1 stride y : 1\n",
      "theta_x shape :  (None, 128, 4, 4)\n",
      "upsample_g shape :  (None, 128, 4, 4)\n",
      "\n",
      "attn1 shape : (None, 64, 8, 8) center shape : (None, 128, 4, 4) \n",
      "\n",
      "attn1 shape : (None, 64, 8, 8) up1 shape : (None, 128, 8, 8)\n",
      "shape x,g  (None, 64, 16, 16) (None, 128, 8, 8)\n",
      "inter shape :   64\n",
      "stride x : 1 stride y : 1\n",
      "theta_x shape :  (None, 64, 8, 8)\n",
      "upsample_g shape :  (None, 64, 8, 8)\n",
      "shape x,g  (None, 32, 32, 32) (None, 128, 16, 16)\n",
      "inter shape :   64\n",
      "stride x : 1 stride y : 1\n",
      "theta_x shape :  (None, 64, 16, 16)\n",
      "upsample_g shape :  (None, 64, 16, 16)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1, 64, 64)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 64, 64)   320         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 64, 64)   256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 64, 64)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 64, 64)   9248        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 64, 64)   256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 64, 64)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 32)   0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 32)   9248        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 32)   128         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 32)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 32)   9248        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 32)   128         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 32)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 32, 16, 16)   0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 64, 16, 16)   18496       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 64, 16, 16)   64          conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 64, 16, 16)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 64, 16, 16)   36928       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 64, 16, 16)   64          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 64, 16, 16)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 64, 8, 8)     0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 64, 8, 8)     36928       max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 64, 8, 8)     32          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 64, 8, 8)     0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 64, 8, 8)     36928       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 64, 8, 8)     32          conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 64, 8, 8)     0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 64, 4, 4)     0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 128, 4, 4)    73856       max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 128, 4, 4)    16          conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 128, 4, 4)    0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 128, 4, 4)    147584      activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 128, 4, 4)    16          conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 128, 4, 4)    0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "gating01_conv (Conv2D)          (None, 128, 4, 4)    16512       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "gating01_bn (BatchNormalization (None, 128, 4, 4)    16          gating01_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "gating01_act (Activation)       (None, 128, 4, 4)    0           gating01_bn[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 128, 4, 4)    16512       gating01_act[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "g_upattn01 (Conv2DTranspose)    (None, 128, 4, 4)    147584      conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "xlattn01 (Conv2D)               (None, 128, 4, 4)    32896       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 128, 4, 4)    0           g_upattn01[0][0]                 \n",
      "                                                                 xlattn01[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 128, 4, 4)    0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "psiattn01 (Conv2D)              (None, 1, 4, 4)      129         activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 1, 4, 4)      0           psiattn01[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 1, 8, 8)      0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "psi_upattn01 (Lambda)           (None, 64, 8, 8)     0           up_sampling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "q_attnattn01 (Multiply)         (None, 64, 8, 8)     0           psi_upattn01[0][0]               \n",
      "                                                                 activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "q_attn_convattn01 (Conv2D)      (None, 64, 8, 8)     4160        q_attnattn01[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 64, 8, 8)     73792       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "q_attn_bnattn01 (BatchNormaliza (None, 64, 8, 8)     32          q_attn_convattn01[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 128, 8, 8)    0           conv2d_transpose_1[0][0]         \n",
      "                                                                 q_attn_bnattn01[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "gating02_conv (Conv2D)          (None, 128, 8, 8)    16512       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "gating02_bn (BatchNormalization (None, 128, 8, 8)    32          gating02_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "gating02_act (Activation)       (None, 128, 8, 8)    0           gating02_bn[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 64, 8, 8)     8256        gating02_act[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "g_upattn02 (Conv2DTranspose)    (None, 64, 8, 8)     36928       conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "xlattn02 (Conv2D)               (None, 64, 8, 8)     16448       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 64, 8, 8)     0           g_upattn02[0][0]                 \n",
      "                                                                 xlattn02[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 64, 8, 8)     0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "psiattn02 (Conv2D)              (None, 1, 8, 8)      65          activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 1, 8, 8)      0           psiattn02[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 1, 16, 16)    0           activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "psi_upattn02 (Lambda)           (None, 64, 16, 16)   0           up_sampling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "q_attnattn02 (Multiply)         (None, 64, 16, 16)   0           psi_upattn02[0][0]               \n",
      "                                                                 activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "q_attn_convattn02 (Conv2D)      (None, 64, 16, 16)   4160        q_attnattn02[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 64, 16, 16)   73792       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "q_attn_bnattn02 (BatchNormaliza (None, 64, 16, 16)   64          q_attn_convattn02[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 128, 16, 16)  0           conv2d_transpose_2[0][0]         \n",
      "                                                                 q_attn_bnattn02[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "gating03_conv (Conv2D)          (None, 128, 16, 16)  16512       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "gating03_bn (BatchNormalization (None, 128, 16, 16)  64          gating03_conv[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "gating03_act (Activation)       (None, 128, 16, 16)  0           gating03_bn[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 64, 16, 16)   8256        gating03_act[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "g_upattn03 (Conv2DTranspose)    (None, 64, 16, 16)   36928       conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "xlattn03 (Conv2D)               (None, 64, 16, 16)   8256        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 64, 16, 16)   0           g_upattn03[0][0]                 \n",
      "                                                                 xlattn03[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 64, 16, 16)   0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "psiattn03 (Conv2D)              (None, 1, 16, 16)    65          activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 1, 16, 16)    0           psiattn03[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)  (None, 1, 32, 32)    0           activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "psi_upattn03 (Lambda)           (None, 32, 32, 32)   0           up_sampling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "q_attnattn03 (Multiply)         (None, 32, 32, 32)   0           psi_upattn03[0][0]               \n",
      "                                                                 activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "q_attn_convattn03 (Conv2D)      (None, 32, 32, 32)   1056        q_attnattn03[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 32, 32, 32)   36896       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "q_attn_bnattn03 (BatchNormaliza (None, 32, 32, 32)   128         q_attn_convattn03[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 64, 32, 32)   0           conv2d_transpose_3[0][0]         \n",
      "                                                                 q_attn_bnattn03[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTrans (None, 32, 64, 64)   18464       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 64, 64, 64)   0           conv2d_transpose_4[0][0]         \n",
      "                                                                 activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 3, 64, 64)    195         concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 3, 4096)      0           conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "permute_1 (Permute)             (None, 4096, 3)      0           reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 4096, 3)      0           permute_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 954,486\n",
      "Trainable params: 953,822\n",
      "Non-trainable params: 664\n",
      "__________________________________________________________________________________________________\n",
      "Check: final output of the network:\n",
      "(None, 4096, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "41736"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_ch = patches_imgs_train.shape[1]\n",
    "patch_height = patches_imgs_train.shape[2]\n",
    "patch_width = patches_imgs_train.shape[3]\n",
    "print('shape : ',patches_imgs_train.shape)\n",
    "print('n_ch : {} patch_h : {} patch_w : {}'.format(n_ch, patch_height, patch_width))\n",
    "#model = naive_attn_unet(n_ch, patch_height, patch_width)  #the U-net model\n",
    "#model = unet_norm(n_ch, patch_height, patch_width,len(mapping))\n",
    "#model = naive_attn_unet(n_ch, patch_height, patch_width,len(mapping))\n",
    "\n",
    "#model = bigger_unet_norm(n_ch, patch_height, patch_width,len(mapping))\n",
    "#model = bigger_naive_attn_unet(n_ch, patch_height, patch_width,len(mapping))\n",
    "model = class3_attn_unet(n_ch, patch_height, patch_width,len(mapping))\n",
    "#model = attn_reg_test(n_ch, patch_height, patch_width,len(mapping))\n",
    "\n",
    "print (\"Check: final output of the network:\")\n",
    "print (model.output_shape)\n",
    "\n",
    "#plot(model, to_file= './'+save_folder+'/'+name_experiment+'/' +name_experiment+ '_model.png')   #check how the model looks like\n",
    "#plot(model, to_file= name_experiment+'/'+name_experiment + '_model.png')   #check how the model looks like\n",
    "\n",
    "json_string = model.to_json()\n",
    "open('./'+save_folder+'/'+name_experiment+'/' +name_experiment+'_architecture.json', 'w').write(json_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[training session] mask unet func patch mask shape :q  (240000, 3, 64, 64)\n",
      "[training session] After mask unet func patch mask shape :  (240000, 4096, 3)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Keras provides a set of functions called callbacks: \n",
    "you can think of callbacks as events that will be triggered at certain training states. \n",
    "The callback we need for checkpointing is the ModelCheckpoint \n",
    "which provides all the features we need according to the checkpointing strategy we adopted in our example\n",
    "'''\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from keras.callbacks import Callback\n",
    "import math\n",
    "\n",
    "class CosineAnnealingScheduler(Callback):\n",
    "    \"\"\"Cosine annealing scheduler.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, T_max, eta_max, eta_min=0, verbose=0):\n",
    "        super(CosineAnnealingScheduler, self).__init__()\n",
    "        self.T_max = T_max\n",
    "        self.eta_max = eta_max\n",
    "        self.eta_min = eta_min\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if not hasattr(self.model.optimizer, 'lr'):\n",
    "            raise ValueError('Optimizer must have a \"lr\" attribute.')\n",
    "        # 1e-9 + (1e-6 ) * (1 + cos ())\n",
    "        lr = self.eta_min + (self.eta_max - self.eta_min) * (1 + math.cos(math.pi * epoch / self.T_max)) / 2\n",
    "        K.set_value(self.model.optimizer.lr, lr)\n",
    "        if self.verbose > 0:\n",
    "            print('\\nEpoch %05d: CosineAnnealingScheduler setting learning '\n",
    "                  'rate to %s.' % (epoch + 1, lr))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        logs['lr'] = K.get_value(self.model.optimizer.lr)\n",
    "        \n",
    "checkpointer = ModelCheckpoint(filepath='./'+save_folder+'/'+name_experiment+'/best_weights.h5', verbose=1, monitor='val_loss', mode='auto', save_best_only=True) #save at each epoch if the validation decreased\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=15, verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=40, verbose=1)\n",
    "\n",
    "print('[training session] mask unet func patch mask shape :q ',patches_masks_train.shape)\n",
    "patches_masks_train = np.reshape(patches_masks_train, (patches_masks_train.shape[0], len(mapping), patch_height* patch_width))\n",
    "patches_masks_train = np.transpose(patches_masks_train,(0,2,1))\n",
    "#patches_masks_train = masks_Unet(patches_masks_train)  #reduce memory consumption\n",
    "print('[training session] After mask unet func patch mask shape : ',patches_masks_train.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240000, 1, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(patches_imgs_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 192000 samples, validate on 48000 samples\n",
      "Epoch 1/200\n",
      "192000/192000 [==============================] - 819s 4ms/step - loss: 12.3379 - generalized_dice_coeff: 0.6958 - val_loss: 7.0606 - val_generalized_dice_coeff: 0.7213\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 7.06064, saving model to ./result/20_0621_fine_segmentor_180_Data_focal_gamma_07/best_weights.h5\n",
      "Epoch 2/200\n",
      "192000/192000 [==============================] - 820s 4ms/step - loss: 6.1894 - generalized_dice_coeff: 0.7426 - val_loss: 5.5164 - val_generalized_dice_coeff: 0.7418\n",
      "\n",
      "Epoch 00002: val_loss improved from 7.06064 to 5.51643, saving model to ./result/20_0621_fine_segmentor_180_Data_focal_gamma_07/best_weights.h5\n",
      "Epoch 3/200\n",
      "192000/192000 [==============================] - 829s 4ms/step - loss: 5.3544 - generalized_dice_coeff: 0.7589 - val_loss: 5.0845 - val_generalized_dice_coeff: 0.7700\n",
      "\n",
      "Epoch 00003: val_loss improved from 5.51643 to 5.08448, saving model to ./result/20_0621_fine_segmentor_180_Data_focal_gamma_07/best_weights.h5\n",
      "Epoch 4/200\n",
      "192000/192000 [==============================] - 820s 4ms/step - loss: 4.9558 - generalized_dice_coeff: 0.7678 - val_loss: 4.9146 - val_generalized_dice_coeff: 0.7823\n",
      "\n",
      "Epoch 00004: val_loss improved from 5.08448 to 4.91456, saving model to ./result/20_0621_fine_segmentor_180_Data_focal_gamma_07/best_weights.h5\n",
      "Epoch 5/200\n",
      "192000/192000 [==============================] - 821s 4ms/step - loss: 4.6955 - generalized_dice_coeff: 0.7743 - val_loss: 4.5820 - val_generalized_dice_coeff: 0.7716\n",
      "\n",
      "Epoch 00005: val_loss improved from 4.91456 to 4.58202, saving model to ./result/20_0621_fine_segmentor_180_Data_focal_gamma_07/best_weights.h5\n",
      "Epoch 6/200\n",
      "192000/192000 [==============================] - 821s 4ms/step - loss: 4.5081 - generalized_dice_coeff: 0.7801 - val_loss: 4.4832 - val_generalized_dice_coeff: 0.7913\n",
      "\n",
      "Epoch 00006: val_loss improved from 4.58202 to 4.48324, saving model to ./result/20_0621_fine_segmentor_180_Data_focal_gamma_07/best_weights.h5\n",
      "Epoch 7/200\n",
      "192000/192000 [==============================] - 820s 4ms/step - loss: 4.3613 - generalized_dice_coeff: 0.7854 - val_loss: 4.3379 - val_generalized_dice_coeff: 0.7944\n",
      "\n",
      "Epoch 00007: val_loss improved from 4.48324 to 4.33790, saving model to ./result/20_0621_fine_segmentor_180_Data_focal_gamma_07/best_weights.h5\n",
      "Epoch 8/200\n",
      "192000/192000 [==============================] - 818s 4ms/step - loss: 4.2441 - generalized_dice_coeff: 0.7902 - val_loss: 4.3513 - val_generalized_dice_coeff: 0.7982\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 4.33790\n",
      "Epoch 9/200\n",
      "192000/192000 [==============================] - 816s 4ms/step - loss: 4.1433 - generalized_dice_coeff: 0.7945 - val_loss: 4.2010 - val_generalized_dice_coeff: 0.8023\n",
      "\n",
      "Epoch 00009: val_loss improved from 4.33790 to 4.20103, saving model to ./result/20_0621_fine_segmentor_180_Data_focal_gamma_07/best_weights.h5\n",
      "Epoch 10/200\n",
      "192000/192000 [==============================] - 814s 4ms/step - loss: 4.0581 - generalized_dice_coeff: 0.7988 - val_loss: 4.1350 - val_generalized_dice_coeff: 0.7999\n",
      "\n",
      "Epoch 00010: val_loss improved from 4.20103 to 4.13501, saving model to ./result/20_0621_fine_segmentor_180_Data_focal_gamma_07/best_weights.h5\n",
      "Epoch 11/200\n",
      "192000/192000 [==============================] - 818s 4ms/step - loss: 3.9847 - generalized_dice_coeff: 0.8026 - val_loss: 4.0778 - val_generalized_dice_coeff: 0.7967\n",
      "\n",
      "Epoch 00011: val_loss improved from 4.13501 to 4.07781, saving model to ./result/20_0621_fine_segmentor_180_Data_focal_gamma_07/best_weights.h5\n",
      "Epoch 12/200\n",
      "192000/192000 [==============================] - 817s 4ms/step - loss: 3.9208 - generalized_dice_coeff: 0.8061 - val_loss: 4.0480 - val_generalized_dice_coeff: 0.8129\n",
      "\n",
      "Epoch 00012: val_loss improved from 4.07781 to 4.04803, saving model to ./result/20_0621_fine_segmentor_180_Data_focal_gamma_07/best_weights.h5\n",
      "Epoch 13/200\n",
      "192000/192000 [==============================] - 816s 4ms/step - loss: 3.8642 - generalized_dice_coeff: 0.8096 - val_loss: 4.0559 - val_generalized_dice_coeff: 0.8078\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 4.04803\n",
      "Epoch 14/200\n",
      "192000/192000 [==============================] - 815s 4ms/step - loss: 3.8155 - generalized_dice_coeff: 0.8126 - val_loss: 3.9980 - val_generalized_dice_coeff: 0.7988\n",
      "\n",
      "Epoch 00014: val_loss improved from 4.04803 to 3.99796, saving model to ./result/20_0621_fine_segmentor_180_Data_focal_gamma_07/best_weights.h5\n",
      "Epoch 15/200\n",
      "192000/192000 [==============================] - 814s 4ms/step - loss: 3.7650 - generalized_dice_coeff: 0.8158 - val_loss: 3.9623 - val_generalized_dice_coeff: 0.8105\n",
      "\n",
      "Epoch 00015: val_loss improved from 3.99796 to 3.96228, saving model to ./result/20_0621_fine_segmentor_180_Data_focal_gamma_07/best_weights.h5\n",
      "Epoch 16/200\n",
      "192000/192000 [==============================] - 814s 4ms/step - loss: 3.7232 - generalized_dice_coeff: 0.8187 - val_loss: 4.0783 - val_generalized_dice_coeff: 0.8213\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 3.96228\n",
      "Epoch 17/200\n",
      "192000/192000 [==============================] - 814s 4ms/step - loss: 3.6824 - generalized_dice_coeff: 0.8212 - val_loss: 3.9092 - val_generalized_dice_coeff: 0.8212\n",
      "\n",
      "Epoch 00017: val_loss improved from 3.96228 to 3.90919, saving model to ./result/20_0621_fine_segmentor_180_Data_focal_gamma_07/best_weights.h5\n",
      "Epoch 18/200\n",
      "192000/192000 [==============================] - 815s 4ms/step - loss: 3.6432 - generalized_dice_coeff: 0.8240 - val_loss: 3.8731 - val_generalized_dice_coeff: 0.8142\n",
      "\n",
      "Epoch 00018: val_loss improved from 3.90919 to 3.87310, saving model to ./result/20_0621_fine_segmentor_180_Data_focal_gamma_07/best_weights.h5\n",
      "Epoch 19/200\n",
      "192000/192000 [==============================] - 809s 4ms/step - loss: 3.6055 - generalized_dice_coeff: 0.8264 - val_loss: 3.8873 - val_generalized_dice_coeff: 0.8229\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 3.87310\n",
      "Epoch 20/200\n",
      "192000/192000 [==============================] - 808s 4ms/step - loss: 3.5732 - generalized_dice_coeff: 0.8288 - val_loss: 3.8636 - val_generalized_dice_coeff: 0.8308\n",
      "\n",
      "Epoch 00020: val_loss improved from 3.87310 to 3.86364, saving model to ./result/20_0621_fine_segmentor_180_Data_focal_gamma_07/best_weights.h5\n",
      "Epoch 21/200\n",
      "192000/192000 [==============================] - 810s 4ms/step - loss: 3.5446 - generalized_dice_coeff: 0.8310 - val_loss: 3.8116 - val_generalized_dice_coeff: 0.8263\n",
      "\n",
      "Epoch 00021: val_loss improved from 3.86364 to 3.81156, saving model to ./result/20_0621_fine_segmentor_180_Data_focal_gamma_07/best_weights.h5\n",
      "Epoch 22/200\n",
      "192000/192000 [==============================] - 809s 4ms/step - loss: 3.5128 - generalized_dice_coeff: 0.8333 - val_loss: 3.8213 - val_generalized_dice_coeff: 0.8255\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 3.81156\n",
      "Epoch 23/200\n",
      "192000/192000 [==============================] - 810s 4ms/step - loss: 3.4835 - generalized_dice_coeff: 0.8353 - val_loss: 3.7860 - val_generalized_dice_coeff: 0.8270\n",
      "\n",
      "Epoch 00023: val_loss improved from 3.81156 to 3.78599, saving model to ./result/20_0621_fine_segmentor_180_Data_focal_gamma_07/best_weights.h5\n",
      "Epoch 24/200\n",
      "192000/192000 [==============================] - 810s 4ms/step - loss: 3.4541 - generalized_dice_coeff: 0.8373 - val_loss: 3.7969 - val_generalized_dice_coeff: 0.8427\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 3.78599\n",
      "Epoch 25/200\n",
      "192000/192000 [==============================] - 810s 4ms/step - loss: 3.4303 - generalized_dice_coeff: 0.8392 - val_loss: 3.8838 - val_generalized_dice_coeff: 0.8413\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 3.78599\n",
      "Epoch 26/200\n",
      "192000/192000 [==============================] - 804s 4ms/step - loss: 3.4063 - generalized_dice_coeff: 0.8412 - val_loss: 3.8147 - val_generalized_dice_coeff: 0.8522\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 3.78599\n",
      "Epoch 27/200\n",
      "192000/192000 [==============================] - 806s 4ms/step - loss: 3.3810 - generalized_dice_coeff: 0.8427 - val_loss: 3.7935 - val_generalized_dice_coeff: 0.8371\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 3.78599\n",
      "Epoch 28/200\n",
      "192000/192000 [==============================] - 805s 4ms/step - loss: 3.3573 - generalized_dice_coeff: 0.8445 - val_loss: 3.7714 - val_generalized_dice_coeff: 0.8460\n",
      "\n",
      "Epoch 00028: val_loss improved from 3.78599 to 3.77139, saving model to ./result/20_0621_fine_segmentor_180_Data_focal_gamma_07/best_weights.h5\n",
      "Epoch 29/200\n",
      "192000/192000 [==============================] - 803s 4ms/step - loss: 3.3375 - generalized_dice_coeff: 0.8459 - val_loss: 3.7955 - val_generalized_dice_coeff: 0.8389\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 3.77139\n",
      "Epoch 30/200\n",
      "192000/192000 [==============================] - 805s 4ms/step - loss: 3.3134 - generalized_dice_coeff: 0.8476 - val_loss: 3.7630 - val_generalized_dice_coeff: 0.8490\n",
      "\n",
      "Epoch 00030: val_loss improved from 3.77139 to 3.76301, saving model to ./result/20_0621_fine_segmentor_180_Data_focal_gamma_07/best_weights.h5\n",
      "Epoch 31/200\n",
      "192000/192000 [==============================] - 808s 4ms/step - loss: 3.2951 - generalized_dice_coeff: 0.8491 - val_loss: 3.7670 - val_generalized_dice_coeff: 0.8515\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 3.76301\n",
      "Epoch 32/200\n",
      "192000/192000 [==============================] - 806s 4ms/step - loss: 3.2780 - generalized_dice_coeff: 0.8504 - val_loss: 3.8235 - val_generalized_dice_coeff: 0.8484\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 3.76301\n",
      "Epoch 33/200\n",
      "192000/192000 [==============================] - 806s 4ms/step - loss: 3.2585 - generalized_dice_coeff: 0.8519 - val_loss: 3.9231 - val_generalized_dice_coeff: 0.8547\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 3.76301\n",
      "Epoch 34/200\n",
      "192000/192000 [==============================] - 802s 4ms/step - loss: 3.2362 - generalized_dice_coeff: 0.8533 - val_loss: 3.7994 - val_generalized_dice_coeff: 0.8517\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 3.76301\n",
      "Epoch 35/200\n",
      "192000/192000 [==============================] - 803s 4ms/step - loss: 3.2221 - generalized_dice_coeff: 0.8545 - val_loss: 3.7548 - val_generalized_dice_coeff: 0.8555\n",
      "\n",
      "Epoch 00035: val_loss improved from 3.76301 to 3.75479, saving model to ./result/20_0621_fine_segmentor_180_Data_focal_gamma_07/best_weights.h5\n",
      "Epoch 36/200\n",
      "192000/192000 [==============================] - 805s 4ms/step - loss: 3.2021 - generalized_dice_coeff: 0.8559 - val_loss: 3.7916 - val_generalized_dice_coeff: 0.8532\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 3.75479\n",
      "Epoch 37/200\n",
      "192000/192000 [==============================] - 805s 4ms/step - loss: 3.1857 - generalized_dice_coeff: 0.8570 - val_loss: 3.7472 - val_generalized_dice_coeff: 0.8490\n",
      "\n",
      "Epoch 00037: val_loss improved from 3.75479 to 3.74719, saving model to ./result/20_0621_fine_segmentor_180_Data_focal_gamma_07/best_weights.h5\n",
      "Epoch 38/200\n",
      "192000/192000 [==============================] - 806s 4ms/step - loss: 3.1695 - generalized_dice_coeff: 0.8581 - val_loss: 3.8325 - val_generalized_dice_coeff: 0.8590\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 3.74719\n",
      "Epoch 39/200\n",
      "192000/192000 [==============================] - 803s 4ms/step - loss: 3.1547 - generalized_dice_coeff: 0.8593 - val_loss: 3.8936 - val_generalized_dice_coeff: 0.8570\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 3.74719\n",
      "Epoch 40/200\n",
      "192000/192000 [==============================] - 802s 4ms/step - loss: 3.1352 - generalized_dice_coeff: 0.8607 - val_loss: 3.8601 - val_generalized_dice_coeff: 0.8621\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 3.74719\n",
      "Epoch 41/200\n",
      "192000/192000 [==============================] - 801s 4ms/step - loss: 3.1222 - generalized_dice_coeff: 0.8616 - val_loss: 3.9190 - val_generalized_dice_coeff: 0.8597\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 3.74719\n",
      "Epoch 42/200\n",
      "192000/192000 [==============================] - 803s 4ms/step - loss: 3.1044 - generalized_dice_coeff: 0.8629 - val_loss: 3.7738 - val_generalized_dice_coeff: 0.8576\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 3.74719\n",
      "Epoch 43/200\n",
      "192000/192000 [==============================] - 802s 4ms/step - loss: 3.0894 - generalized_dice_coeff: 0.8638 - val_loss: 3.8070 - val_generalized_dice_coeff: 0.8632\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 3.74719\n",
      "Epoch 44/200\n",
      "192000/192000 [==============================] - 800s 4ms/step - loss: 3.0771 - generalized_dice_coeff: 0.8648 - val_loss: 3.9077 - val_generalized_dice_coeff: 0.8681\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 3.74719\n",
      "Epoch 45/200\n",
      "192000/192000 [==============================] - 800s 4ms/step - loss: 3.0625 - generalized_dice_coeff: 0.8658 - val_loss: 3.7894 - val_generalized_dice_coeff: 0.8606\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 3.74719\n",
      "Epoch 46/200\n",
      "192000/192000 [==============================] - 801s 4ms/step - loss: 3.0472 - generalized_dice_coeff: 0.8667 - val_loss: 3.8787 - val_generalized_dice_coeff: 0.8563\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 3.74719\n",
      "Epoch 47/200\n",
      "192000/192000 [==============================] - 802s 4ms/step - loss: 3.0380 - generalized_dice_coeff: 0.8674 - val_loss: 3.7543 - val_generalized_dice_coeff: 0.8602\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 3.74719\n",
      "Epoch 48/200\n",
      "192000/192000 [==============================] - 802s 4ms/step - loss: 3.0243 - generalized_dice_coeff: 0.8684 - val_loss: 3.9489 - val_generalized_dice_coeff: 0.8657\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 3.74719\n",
      "Epoch 49/200\n",
      "192000/192000 [==============================] - 802s 4ms/step - loss: 3.0131 - generalized_dice_coeff: 0.8691 - val_loss: 3.7796 - val_generalized_dice_coeff: 0.8639\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 3.74719\n",
      "Epoch 50/200\n",
      "192000/192000 [==============================] - 803s 4ms/step - loss: 2.9965 - generalized_dice_coeff: 0.8702 - val_loss: 3.9590 - val_generalized_dice_coeff: 0.8680\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 3.74719\n",
      "Epoch 51/200\n",
      "192000/192000 [==============================] - 804s 4ms/step - loss: 2.9853 - generalized_dice_coeff: 0.8709 - val_loss: 3.9551 - val_generalized_dice_coeff: 0.8712\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 3.74719\n",
      "Epoch 52/200\n",
      "192000/192000 [==============================] - 802s 4ms/step - loss: 2.9758 - generalized_dice_coeff: 0.8717 - val_loss: 3.8460 - val_generalized_dice_coeff: 0.8648\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 3.74719\n",
      "Epoch 53/200\n",
      "192000/192000 [==============================] - 800s 4ms/step - loss: 2.9639 - generalized_dice_coeff: 0.8725 - val_loss: 3.9187 - val_generalized_dice_coeff: 0.8724\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 3.74719\n",
      "Epoch 54/200\n",
      "192000/192000 [==============================] - 803s 4ms/step - loss: 2.9524 - generalized_dice_coeff: 0.8733 - val_loss: 3.8682 - val_generalized_dice_coeff: 0.8707\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 3.74719\n",
      "Epoch 55/200\n",
      "192000/192000 [==============================] - 799s 4ms/step - loss: 2.9404 - generalized_dice_coeff: 0.8740 - val_loss: 4.0354 - val_generalized_dice_coeff: 0.8747\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 3.74719\n",
      "Epoch 56/200\n",
      "192000/192000 [==============================] - 797s 4ms/step - loss: 2.9299 - generalized_dice_coeff: 0.8747 - val_loss: 3.9829 - val_generalized_dice_coeff: 0.8720\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 3.74719\n",
      "Epoch 57/200\n",
      "192000/192000 [==============================] - 801s 4ms/step - loss: 2.9197 - generalized_dice_coeff: 0.8754 - val_loss: 3.9143 - val_generalized_dice_coeff: 0.8742\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 3.74719\n",
      "Epoch 58/200\n",
      "192000/192000 [==============================] - 798s 4ms/step - loss: 2.9074 - generalized_dice_coeff: 0.8763 - val_loss: 3.8761 - val_generalized_dice_coeff: 0.8723\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 3.74719\n",
      "Epoch 59/200\n",
      "192000/192000 [==============================] - 795s 4ms/step - loss: 2.8998 - generalized_dice_coeff: 0.8769 - val_loss: 4.0525 - val_generalized_dice_coeff: 0.8768\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 3.74719\n",
      "Epoch 60/200\n",
      "192000/192000 [==============================] - 795s 4ms/step - loss: 2.8890 - generalized_dice_coeff: 0.8776 - val_loss: 4.0207 - val_generalized_dice_coeff: 0.8746\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 3.74719\n",
      "Epoch 61/200\n",
      "192000/192000 [==============================] - 798s 4ms/step - loss: 2.8794 - generalized_dice_coeff: 0.8783 - val_loss: 3.9046 - val_generalized_dice_coeff: 0.8732\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 3.74719\n",
      "Epoch 62/200\n",
      "192000/192000 [==============================] - 799s 4ms/step - loss: 2.8682 - generalized_dice_coeff: 0.8790 - val_loss: 3.9007 - val_generalized_dice_coeff: 0.8715\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 3.74719\n",
      "Epoch 63/200\n",
      "192000/192000 [==============================] - 802s 4ms/step - loss: 2.8599 - generalized_dice_coeff: 0.8795 - val_loss: 3.9424 - val_generalized_dice_coeff: 0.8740\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 3.74719\n",
      "Epoch 64/200\n",
      "192000/192000 [==============================] - 804s 4ms/step - loss: 2.8497 - generalized_dice_coeff: 0.8802 - val_loss: 3.8862 - val_generalized_dice_coeff: 0.8725\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 3.74719\n",
      "Epoch 65/200\n",
      "192000/192000 [==============================] - 802s 4ms/step - loss: 2.8405 - generalized_dice_coeff: 0.8810 - val_loss: 3.9566 - val_generalized_dice_coeff: 0.8720\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 3.74719\n",
      "Epoch 66/200\n",
      "192000/192000 [==============================] - 782s 4ms/step - loss: 2.8324 - generalized_dice_coeff: 0.8816 - val_loss: 4.0293 - val_generalized_dice_coeff: 0.8790\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 3.74719\n",
      "Epoch 67/200\n",
      "192000/192000 [==============================] - 772s 4ms/step - loss: 2.8244 - generalized_dice_coeff: 0.8821 - val_loss: 4.0144 - val_generalized_dice_coeff: 0.8800\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 3.74719\n",
      "Epoch 68/200\n",
      "192000/192000 [==============================] - 806s 4ms/step - loss: 2.8139 - generalized_dice_coeff: 0.8828 - val_loss: 3.9632 - val_generalized_dice_coeff: 0.8749\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 3.74719\n",
      "Epoch 69/200\n",
      "192000/192000 [==============================] - 805s 4ms/step - loss: 2.8067 - generalized_dice_coeff: 0.8832 - val_loss: 4.0704 - val_generalized_dice_coeff: 0.8784\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 3.74719\n",
      "Epoch 70/200\n",
      "192000/192000 [==============================] - 791s 4ms/step - loss: 2.7959 - generalized_dice_coeff: 0.8840 - val_loss: 4.2002 - val_generalized_dice_coeff: 0.8836\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 3.74719\n",
      "Epoch 71/200\n",
      "192000/192000 [==============================] - 804s 4ms/step - loss: 2.7883 - generalized_dice_coeff: 0.8844 - val_loss: 4.1102 - val_generalized_dice_coeff: 0.8791\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 3.74719\n",
      "Epoch 72/200\n",
      "192000/192000 [==============================] - 804s 4ms/step - loss: 2.7814 - generalized_dice_coeff: 0.8848 - val_loss: 4.2124 - val_generalized_dice_coeff: 0.8846\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 3.74719\n",
      "Epoch 73/200\n",
      "192000/192000 [==============================] - 787s 4ms/step - loss: 2.7732 - generalized_dice_coeff: 0.8855 - val_loss: 4.3233 - val_generalized_dice_coeff: 0.8876\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 3.74719\n",
      "Epoch 74/200\n",
      "192000/192000 [==============================] - 808s 4ms/step - loss: 2.7652 - generalized_dice_coeff: 0.8859 - val_loss: 4.0511 - val_generalized_dice_coeff: 0.8779\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 3.74719\n",
      "Epoch 75/200\n",
      "192000/192000 [==============================] - 752s 4ms/step - loss: 2.7571 - generalized_dice_coeff: 0.8865 - val_loss: 4.2276 - val_generalized_dice_coeff: 0.8832\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 3.74719\n",
      "Epoch 76/200\n",
      "192000/192000 [==============================] - 743s 4ms/step - loss: 2.7494 - generalized_dice_coeff: 0.8872 - val_loss: 4.2282 - val_generalized_dice_coeff: 0.8857\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 3.74719\n",
      "Epoch 77/200\n",
      "192000/192000 [==============================] - 774s 4ms/step - loss: 2.7433 - generalized_dice_coeff: 0.8874 - val_loss: 4.2982 - val_generalized_dice_coeff: 0.8850\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 3.74719\n",
      "Epoch 00077: early stopping\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    history = model.fit(patches_imgs_train, patches_masks_train, epochs=num_epochs, batch_size=batch_size, verbose=1, shuffle=True, validation_split=0.2, callbacks=[checkpointer,CosineAnnealingScheduler(T_max=200, eta_max=5e-6, eta_min=1e-7),early_stopping])\n",
    "    model.save_weights('./'+save_folder+'/'+name_experiment +'/last_weights.h5', overwrite=False)\n",
    "except KeyboardInterrupt:\n",
    "    model.save_weights('./'+save_folder+'/'+name_experiment +'/last_weights.h5', overwrite=False)\n",
    "    print('Keyboard Interrupt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'generalized_dice_coeff', 'val_generalized_dice_coeff', 'lr', 'loss'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already exist the folder in this path : ./result/20_0621_fine_segmentor_180_Data_focal_gamma_07/train_history\n",
      "already exist the folder in this path : ./result/20_0621_fine_segmentor_180_Data_focal_gamma_07/train_history\n",
      "already exist the folder in this path : ./result/20_0621_fine_segmentor_180_Data_focal_gamma_07/train_history\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(history.history.keys())\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss= history.history['val_loss']\n",
    "acc = history.history['generalized_dice_coeff']\n",
    "val_acc = history.history['val_generalized_dice_coeff']\n",
    "\n",
    "epochs = range(1,len(acc) +1)\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label = \"Training loss\")\n",
    "plt.plot(epochs, val_loss, 'b', label = \"Validation loss\")\n",
    "plt.title(\"Training and Validation loss\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.savefig('./'+save_folder+'/'+name_experiment+\"/training_loss_result.png\")\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, acc, 'ro', label = \"Training generalized_dice_coeff\")\n",
    "plt.plot(epochs, val_acc, 'r', label = \"Validation generalized_dice_coeff\")\n",
    "plt.title(\"Training and Validation dice coef\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel('acc')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.savefig('./'+save_folder+'/'+name_experiment+\"/training_acc_result.png\")\n",
    "plt.show()\n",
    "\n",
    "import pandas as pd\n",
    "file_path = './'+save_folder+'/'+name_experiment + '/' + 'train_history'\n",
    "\n",
    "def save_history_txt_csv(history, file_path, file_name):\n",
    "    if os.path.isdir(file_path) == False:\n",
    "        os.mkdir(file_path)\n",
    "    else:\n",
    "        print('already exist the folder in this path : {}'.format(file_path))\n",
    "    \n",
    "    hist_df = pd.DataFrame(history) \n",
    "\n",
    "    # save to json:  \n",
    "    hist_json_file = file_path + '/' + file_name +'.json' \n",
    "    with open(hist_json_file, mode='w') as f:\n",
    "        hist_df.to_json(f)\n",
    "\n",
    "    # or save to csv: \n",
    "    hist_csv_file = file_path + '/' + file_name + '.csv'\n",
    "    with open(hist_csv_file, mode='w') as f:\n",
    "        hist_df.to_csv(f)\n",
    "        \n",
    "\n",
    "save_history_txt_csv(loss, file_path, 'train_loss')\n",
    "save_history_txt_csv(val_loss, file_path, 'val_loss')\n",
    "save_history_txt_csv(acc, file_path, 'train_acc')\n",
    "save_history_txt_csv(val_acc, file_path, 'val_acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
